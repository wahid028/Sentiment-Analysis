{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wahid028/Sentiment-Analysis/blob/main/1_NLP_Data_pre_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi0tVr3HHLwh"
      },
      "source": [
        "[statinfer.com](https://statinfer.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsIVzVt1QPwv"
      },
      "source": [
        "# Packages Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--Pw52DbG7xf"
      },
      "source": [
        "#!pip install nltk\n",
        "#!pip install spacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vw1CX5Vo8Nkl"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "nltk.download(\"all\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agNljfsEh1u1"
      },
      "source": [
        "# Data Importing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R51forVQaG4"
      },
      "source": [
        "twitter_data=pd.read_csv(\"https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Twitter_Sentiment/Twitter_Sentiment_Data.csv\")\n",
        "twitter_data.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02yn77vyDp8M"
      },
      "source": [
        "#Lower Case conversion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TGLVQdMDVX9"
      },
      "source": [
        "twitter_data[\"tweet_lowcase\"]=twitter_data[\"raw_tweet\"].apply(lambda x:str(x).lower())\n",
        "twitter_data.sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y_ZtCu98uI9"
      },
      "source": [
        "#Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSBUg9qo8WMp"
      },
      "source": [
        "from nltk.tokenize import word_tokenize \n",
        "twitter_data[\"word_tokens\"] = twitter_data[\"tweet_lowcase\"].apply(lambda x:word_tokenize(str(x)))\n",
        "#lambda function to apply on all rows\n",
        "#str() function to avoid numeric and other errors\n",
        "twitter_data[[\"raw_tweet\",\"word_tokens\"]].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nU9K6MKFF0Mm"
      },
      "source": [
        "twitter_data[[\"raw_tweet\",\"word_tokens\"]].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIkkH6XrgYUy"
      },
      "source": [
        "# Expanding shortforms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsnZSnMBgegt"
      },
      "source": [
        "contra_Expan_Dict = {\"ain`t\": \"am not\",\"aren`t\": \"are not\",\"can`t\": \"cannot\",\"can`t`ve\": \"cannot have\",\"`cause\": \"because\",\n",
        "\"could`ve\": \"could have\",\"couldn`t\": \"could not\",\"couldn`t`ve\": \"could not have\",\"didn`t\": \"did not\",\n",
        "\"doesn`t\": \"does not\",\"don`t\": \"do not\",\"hadn`t\": \"had not\",\"hadn`t`ve\": \"had not have\",\"hasn`t\": \"has not\",\n",
        "\"haven`t\": \"have not\",\"he`d\": \"he would\",\"he`d`ve\": \"he would have\",\"he`ll\": \"he will\",\"he`ll`ve\": \"he will have\",\n",
        "\"he`s\": \"he is\",\"how`d\": \"how did\",\"how`d`y\": \"how do you\",\"how`ll\": \"how will\",\n",
        "\"how`s\": \"how does\",\"i`d\": \"i would\",\"i`d`ve\": \"i would have\",\"i`ll\": \"i will\",\"i`ll`ve\": \"i will have\",\"i`m\": \"i am\",\n",
        "\"i`ve\": \"i have\",\"isn`t\": \"is not\",\"it`d\": \"it would\",\"it`d`ve\": \"it would have\",\"it`ll\": \"it will\",\"it`ll`ve\": \"it will have\",\n",
        "\"it`s\": \"it is\",\"let`s\": \"let us\",\"ma`am\": \"madam\",\"mayn`t\": \"may not\",\"might`ve\": \"might have\",\"mightn`t\": \"might not\",\n",
        "\"mightn`t`ve\": \"might not have\",\"must`ve\": \"must have\",\"mustn`t\": \"must not\",\"mustn`t`ve\": \"must not have\",\"needn`t\": \"need not\",\"needn`t`ve\": \"need not have\",\n",
        "\"o`clock\": \"of the clock\",\"oughtn`t\": \"ought not\",\"oughtn`t`ve\": \"ought not have\",\"shan`t\": \"shall not\",\n",
        "\"sha`n`t\": \"shall not\",\"shan`t`ve\": \"shall not have\",\"she`d\": \"she would\",\n",
        "\"she`d`ve\": \"she would have\",\"she`ll\": \"she will\",\"she`ll`ve\": \"she will have\",\n",
        "\"she`s\": \"she is\",\"should`ve\": \"should have\",\"shouldn`t\": \"should not\",\"shouldn`t`ve\": \"should not have\",\"so`ve\": \"so have\",\"so`s\": \"so is\",\n",
        "\"that`d\": \"that would\",\"that`d`ve\": \"that would have\",\"that`s\": \"that is\",\"there`d\": \"there would\",\"there`d`ve\": \"there would have\",\"there`s\": \"there is\",\n",
        "\"they`d\": \"they would\",\"they`d`ve\": \"they would have\",\"they`ll\": \"they will\",\"they`ll`ve\": \"they will have\",\"they`re\": \"they are\",\"they`ve\": \"they have\",\n",
        "\"to`ve\": \"to have\",\"wasn`t\": \"was not\",\" u \": \" you \",\" ur \": \" your \",\" n \": \" and \",\"won`t\": \"would not\",\n",
        "\"dis\": \"this\",\"bak\": \"back\",\"brng\": \"bring\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIC7ZcYSggo_"
      },
      "source": [
        "def expanded_form(x):\n",
        "  if x in contra_Expan_Dict.keys():\n",
        "    return(contra_Expan_Dict[x])\n",
        "  else:\n",
        "    return(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX855ewziU39"
      },
      "source": [
        "x=str(twitter_data[\"tweet_lowcase\"][6207])\n",
        "print(\"original tweet ==>\", x)\n",
        "x=x.split()\n",
        "print(\"Expanded form ==>\",[expanded_form(t) for t in x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coT26ATehSr-"
      },
      "source": [
        "twitter_data[\"tweet_expanded\"]=twitter_data[\"tweet_lowcase\"].apply(lambda x:[expanded_form(t) for t in str(x).split()])\n",
        "twitter_data[[\"raw_tweet\",\"tweet_expanded\"]].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQsgPHv7s5vm"
      },
      "source": [
        "twitter_data[[\"raw_tweet\",\"tweet_expanded\"]].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfVKVm11AYHP"
      },
      "source": [
        "# Stopwords Removal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYtH-zQsBllA"
      },
      "source": [
        "## NLTK Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfBVXZAA_pd2"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "nltk_stop_words = set(stopwords.words('english')) ##Selecting the stop words from the Language\n",
        "print(\"Number of Stop words in NLTK ==>\", len(nltk_stop_words))\n",
        "print(sorted(nltk_stop_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3i71M3OzBq4U"
      },
      "source": [
        "## spaCy Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTTBkct-BSvs"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords\n",
        "print(\"Number of Stop words in spaCy ==>\", len(spacy_stopwords))\n",
        "print(sorted(spacy_stopwords))\n",
        "#Spacy stopwords list looks better. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ6bCCjnuCXh"
      },
      "source": [
        "x=twitter_data[\"tweet_expanded\"][16355]\n",
        "print(\"original tweet ==>\", x)\n",
        "print(\"After Removing Stopwords ==>\",[t for t in x if t not in spacy_stopwords])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGGFHOE8CCfg"
      },
      "source": [
        "twitter_data[\"After_Removing_Stopwords\"] = twitter_data[\"tweet_expanded\"].apply(lambda x:[t for t in x if t not in spacy_stopwords ])\n",
        "twitter_data[[\"raw_tweet\",\"After_Removing_Stopwords\"]].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf4rwc2SmMu6"
      },
      "source": [
        "## Add Custom Stowords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZJOE2ecmMIi"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords\n",
        "spacy_stopwords.update({\"would\", \"rt\",\"like\", \"ha\", \"lol\", \"need\", \"do\"})\n",
        "print(\"New Number of Stop words in spaCy ==>\", len(spacy_stopwords))\n",
        "print(sorted(spacy_stopwords))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dLJR6h-mRS4"
      },
      "source": [
        "twitter_data[\"After_Removing_Stopwords\"] = twitter_data[\"tweet_expanded\"].apply(lambda x:[t for t in x if t not in spacy_stopwords ])\n",
        "twitter_data[[\"raw_tweet\",\"After_Removing_Stopwords\"]].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxNGEKoPWAy9"
      },
      "source": [
        "#Regular Expression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6Sv_Zs3f41L"
      },
      "source": [
        "def clean_with_re(x):\n",
        "  x=str(x)\n",
        "  x=re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\" \", x) #Remove URLs\n",
        "  x=re.sub(r'[^\\w ]+', \"\", x) # Remove Punctuation-1\n",
        "  x=re.sub(r\"[,!@&\\'?\\.$%_]\",\" \", x) # Remove Punctuation-2\n",
        "  x=re.sub(r\"\\d+\",\" \", x) #Remove digits\n",
        "  return(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9G6Mgq0kWzr"
      },
      "source": [
        "import re\n",
        "twitter_data[\"tweet_cleaned_Regex\"]=twitter_data[\"After_Removing_Stopwords\"].apply(lambda x:clean_with_re(x))\n",
        "twitter_data[[\"raw_tweet\",\"tweet_cleaned_Regex\"]].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8--9JEJ-kxir"
      },
      "source": [
        "twitter_data[[\"After_Removing_Stopwords\",\"tweet_cleaned_Regex\"]].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jDncuCJHz1v"
      },
      "source": [
        "#Spelling Correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd3SxQGQC_-n"
      },
      "source": [
        "#!pip install textblob\n",
        "#!python textblob.download_corpora"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXHVZ-8zH9nn"
      },
      "source": [
        "from textblob import TextBlob\n",
        "sample_tweet=\"What an grat and amazimg week. I am excited to learn data scienec\"\n",
        "corrected_tweet=TextBlob(sample_tweet).correct()\n",
        "corrected_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMnQYep1J5Oh"
      },
      "source": [
        "#Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKh4PgKiIf_Y"
      },
      "source": [
        "spacy_model = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CurrMWjJMkmv"
      },
      "source": [
        "sample_tweet=twitter_data[\"tweet_cleaned_Regex\"][0]\n",
        "print(\"Original Text ==>\", sample_tweet)\n",
        "print(\"Lemmatization Results ==>\", \" \".join([t.lemma_ for t in spacy_model(str(sample_tweet))]))\n",
        "#print(\"Lemmatization PRON removed ==>\", \" \".join([t.lemma_ for t in spacy_model(str(sample_tweet)) if t.lemma_ !=\"-PRON-\" ]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GCGTcE3KuGX"
      },
      "source": [
        "twitter_data[\"Lemmatized_tweet\"] = twitter_data[\"tweet_cleaned_Regex\"].apply(lambda x:\" \".join([t.lemma_ for t in spacy_model(str(x))if t.lemma_ !=\"-PRON-\" ]))\n",
        "twitter_data[[\"raw_tweet\",\"Lemmatized_tweet\"]].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0aa7KbPMTQn"
      },
      "source": [
        "#Few more samples\n",
        "twitter_data[[\"raw_tweet\",\"Lemmatized_tweet\"]].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQk9PGLzbvQt"
      },
      "source": [
        "#Any Further Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MHEHnMdb4Qy"
      },
      "source": [
        "spacy_stopwords.update({\"would\", \"rt\",\"like\", \"ha\", \"lol\", \"need\", \"do\"})\n",
        "twitter_data[\"Final_Cleaned_Tweet\"] = twitter_data[\"Lemmatized_tweet\"].apply(lambda x:[t for t in str(x).split() if t not in spacy_stopwords ])\n",
        "twitter_data[\"Final_Cleaned_Tweet_tokens\"]=twitter_data[\"Final_Cleaned_Tweet\"].apply(lambda x: \" \".join(x) )\n",
        "twitter_data[[\"raw_tweet\",\"Final_Cleaned_Tweet_tokens\"]].sample(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfcYzleLB2Ts"
      },
      "source": [
        "# Word Cloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbPOumhDB4HG"
      },
      "source": [
        "#!pip install wordcloud\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjiixYeKCFke"
      },
      "source": [
        "final_text=\"\".join(twitter_data[\"Final_Cleaned_Tweet_tokens\"])\n",
        "len(final_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFjLb2FjCFrV"
      },
      "source": [
        "plt.figure(figsize = (15, 15), facecolor = None) \n",
        "wc=WordCloud(colormap='Set2').generate(final_text)\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJl47bFFmAlJ"
      },
      "source": [
        "## Fancy wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khRaJJnkFRv3"
      },
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import urllib.request \n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/venkatareddykonasani/Datasets/master/Assorted/statinfer-logo-transparent-icon.png\", \"statinfer-logo-transparent_v1.png\")\n",
        "BG_image = np.array(Image.open(\"statinfer-logo-transparent_v1.png\"))\n",
        "plt.imshow(BG_image)\n",
        "plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH0xQGlHDUKq"
      },
      "source": [
        "plt.figure(figsize = (10, 10))\n",
        "wc=WordCloud(mask=BG_image, contour_color='white', contour_width=3).generate(final_text)\n",
        "plt.imshow(wc, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GktjOUmPVXVT"
      },
      "source": [
        "# A Single function for pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dlr0NK_Vjf8"
      },
      "source": [
        "def pre_processing(input_data, text_col):\n",
        "  input_data[\"text_col_clean\"]=input_data[text_col].apply(lambda x:str(x).lower())\n",
        "  input_data[\"text_col_clean\"]=input_data[\"text_col_clean\"].apply(lambda x:[expanded_form(t) for t in str(x).split()])\n",
        "  input_data[\"text_col_clean\"]=input_data[\"text_col_clean\"].apply(lambda x:[t for t in x if t not in spacy_stopwords ])\n",
        "  input_data[\"text_col_clean\"]=input_data[\"text_col_clean\"].apply(lambda x:clean_with_re(x))\n",
        "  input_data[\"text_col_clean\"]=input_data[\"text_col_clean\"].apply(lambda x:\" \".join([t.lemma_ for t in spacy_model(str(x))if t.lemma_ !=\"-PRON-\" ]))\n",
        "  input_data[\"text_col_clean\"]=input_data[\"text_col_clean\"].apply(lambda x:[t for t in str(x).split() if t not in spacy_stopwords ])\n",
        "  input_data[\"text_col_clean\"]=input_data[\"text_col_clean\"].apply(lambda x: \" \".join(x) )\n",
        "  print(input_data[[text_col,\"text_col_clean\"]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdADeVM0YH1G"
      },
      "source": [
        "pre_processing(input_data=twitter_data, text_col=\"raw_tweet\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geG4E23U9XvP"
      },
      "source": [
        "# Document Term Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVo3d4cM9aXl"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "countvec1 = CountVectorizer(min_df= 5) #minimum word freq=5\n",
        "dtm_v1 = pd.DataFrame(countvec1.fit_transform(twitter_data['Final_Cleaned_Tweet_tokens']).toarray(), columns=countvec1.get_feature_names(), index=None)\n",
        "print(dtm_v1.shape)\n",
        "dtm_v1\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}