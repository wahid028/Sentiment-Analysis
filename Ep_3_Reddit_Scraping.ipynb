{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-ANeoUfcnId"
   },
   "source": [
    "# Install PRAW API:\n",
    "### Documentation: https://praw.readthedocs.io/en/latest/ \n",
    "### https://www.youtube.com/watch?v=Y7BSe7EiBTs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5998,
     "status": "ok",
     "timestamp": 1614485490795,
     "user": {
      "displayName": "BitsInBytes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj4_qtDugwTI7x9UjhXAbLPMQvtcyQD3UMj8gLc=s64",
      "userId": "13908254166643482828"
     },
     "user_tz": 360
    },
    "id": "xSjHfXGHAZJ1",
    "outputId": "60305c38-0fac-4276-b24c-5280f400ccef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for soupsieve: [Errno 2] No such file or directory: 'c:\\\\users\\\\seokju gist\\\\anaconda3\\\\envs\\\\mitoenv\\\\lib\\\\site-packages\\\\soupsieve-2.3.2.post1.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for pyzmq: [Errno 2] No such file or directory: 'c:\\\\users\\\\seokju gist\\\\anaconda3\\\\envs\\\\mitoenv\\\\lib\\\\site-packages\\\\pyzmq-23.2.1.dist-info\\\\METADATA'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.6.0-py3-none-any.whl (188 kB)\n",
      "     ------------------------------------ 188.2/188.2 kB 669.0 kB/s eta 0:00:00\n",
      "Collecting prawcore<3,>=2.1\n",
      "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
      "Collecting update-checker>=0.18\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\seokju gist\\anaconda3\\envs\\mitoenv\\lib\\site-packages (from praw) (1.4.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\seokju gist\\anaconda3\\envs\\mitoenv\\lib\\site-packages (from prawcore<3,>=2.1->praw) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\seokju gist\\anaconda3\\envs\\mitoenv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\seokju gist\\anaconda3\\envs\\mitoenv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\seokju gist\\anaconda3\\envs\\mitoenv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\seokju gist\\anaconda3\\envs\\mitoenv\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2022.9.24)\n",
      "Installing collected packages: update-checker, prawcore, praw\n",
      "Successfully installed praw-7.6.0 prawcore-2.3.0 update-checker-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install praw\n",
    "#pip install praw #Jupyter\n",
    "#conda install -c conda-forge praw #Anaconda\n",
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zmeQVvfYgrO"
   },
   "source": [
    "# Get Credentials:\n",
    "https://old.reddit.com/prefs/apps/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 390,
     "status": "ok",
     "timestamp": 1614485552402,
     "user": {
      "displayName": "BitsInBytes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj4_qtDugwTI7x9UjhXAbLPMQvtcyQD3UMj8gLc=s64",
      "userId": "13908254166643482828"
     },
     "user_tz": 360
    },
    "id": "GoEwukL7BLrZ"
   },
   "outputs": [],
   "source": [
    "# Log In to App: \n",
    "reddit = praw.Reddit(client_id='9MKSo0tYTRaUWPrcNq8tpA', \n",
    "                     client_secret='5twxWoEj6Y4fvik1SbqA7Sac105PNg', \n",
    "                     user_agent='price App')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(reddit.read_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorized Reddit Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=\"9MKSo0tYTRaUWPrcNq8tpA\",\n",
    "    client_secret=\"5twxWoEj6Y4fvik1SbqA7Sac105PNg\",\n",
    "    password=\"nitsawahied374\",\n",
    "    user_agent=\"price App\",\n",
    "    username=\"wahid028\",\n",
    ")\n",
    "\n",
    "print(reddit.read_only)\n",
    "# Output: False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wahid028\n"
     ]
    }
   ],
   "source": [
    "print(reddit.user.me())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hztvSe0oYt-v"
   },
   "source": [
    "# Hot Submissions in Subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1614485558714,
     "user": {
      "displayName": "BitsInBytes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj4_qtDugwTI7x9UjhXAbLPMQvtcyQD3UMj8gLc=s64",
      "userId": "13908254166643482828"
     },
     "user_tz": 360
    },
    "id": "k8rCZ9heDTrQ",
    "outputId": "8f1e299c-7c66-4ff0-985a-974b48aa2b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly Optimists Discussion - October 2022\n",
      "Daily General Discussion - October 17, 2022 (GMT+0)\n",
      "Kanye West aka \"Ye\" photographed sporting a Satoshi Nakamoto baseball cap after being terminated as a client by his bank, Jp Morgan.\n",
      "Celsius is currently burning through a deficit of $1.5m a day. Bankruptcy legal counsel charged them $2.5m for 18 days of work (i.e $5800 per hour). All of this comes from user's funds remaining in the bankruptcy estate\n",
      "Bitcoin Whale Moves 500 BTC Dormant Since 5+ Years Ago\n",
      "Anyone else feel like their DCA is just a break even black hole at this point....\n",
      "Polkadot hits all-time high in development activity\n",
      "Man Faces 25 Years Behind Bars for Illegally Converting Bitcoin (BTC) to US Dollars in Money Laundering Scheme - The Daily Hodl\n",
      "North Korea’s Lazarus behind years of crypto hacks in Japan: Police\n",
      "When people use terms like 99% of all crypto will go to $0. They overlook how this is normal in ALL markets\n"
     ]
    }
   ],
   "source": [
    "subs = reddit.subreddit('CryptoCurrency').hot(limit=10)\n",
    "#subs = reddit.subreddit('bangladesh').new(limit=10)\n",
    "#subs = reddit.subreddit('bangladesh').controversial(limit=10)\n",
    "#subs = reddit.subreddit('bangladesh').gilded(limit=10) // It is not working\n",
    "#subs = reddit.subreddit('bangladesh').top(limit=10)\n",
    "#subs = reddit.subreddit('bangladesh').rising(limit=10)\n",
    "\n",
    "for submission in subs:\n",
    "    print(submission.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  score      id subreddit  \\\n",
      "0               Bitcoin Newcomers FAQ - Please read!    177  wds6fs   Bitcoin   \n",
      "1                 Daily Discussion, October 17, 2022      2  y621v5   Bitcoin   \n",
      "2  To those of you still hodling strong... you ar...     43  y5zyfe   Bitcoin   \n",
      "3                              WTF Happened In 1971?    789  y5e7nt   Bitcoin   \n",
      "4      Something for the \"Bitcoin is too slow\" crowd    485  y5f29q   Bitcoin   \n",
      "5  33 BTC were traded in Venezuela last week (mea...     31  y5z01y   Bitcoin   \n",
      "6  Percentage of Canadians who own bitcoin triple...    192  y5kqd8   Bitcoin   \n",
      "7                                  The year is 2052…     24  y5x6aa   Bitcoin   \n",
      "8                  Nakamoto mining the genesis block      6  y64vb6   Bitcoin   \n",
      "9  Got to meet Adam Back in Amsterdam. Sadly I di...    177  y5gewn   Bitcoin   \n",
      "\n",
      "                                                 url  num_comments  \\\n",
      "0  https://www.reddit.com/r/Bitcoin/comments/wds6...            83   \n",
      "1  https://www.reddit.com/r/Bitcoin/comments/y621...            12   \n",
      "2  https://www.reddit.com/r/Bitcoin/comments/y5zy...            16   \n",
      "3                     https://wtfhappenedin1971.com/           309   \n",
      "4                    https://v.redd.it/pww2zmedq5u91            73   \n",
      "5  https://www.reddit.com/r/Bitcoin/comments/y5z0...             1   \n",
      "6                https://i.redd.it/o39wg2k407u91.png            23   \n",
      "7  https://www.reddit.com/r/Bitcoin/comments/y5x6...            12   \n",
      "8                https://i.redd.it/lwu644hanbu91.jpg             1   \n",
      "9                    https://i.imgur.com/wOBlLf0.jpg            55   \n",
      "\n",
      "                                                body       created  \n",
      "0  # Welcome to the /r/Bitcoin Sticky FAQ\\n\\nYou'...  1.659387e+09  \n",
      "1  Please utilize this sticky thread for all gene...  1.665983e+09  \n",
      "2  I just think it's funny how there was all this...  1.665976e+09  \n",
      "3                                                     1.665919e+09  \n",
      "4                                                     1.665922e+09  \n",
      "5  Hi guys, just posting this to keep the traditi...  1.665974e+09  \n",
      "6                                                     1.665937e+09  \n",
      "7  The year is 2052…\\n\\nYou turn on the tv to see...  1.665968e+09  \n",
      "8                                                     1.665993e+09  \n",
      "9                                                     1.665926e+09  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "posts = []\n",
    "ml_subreddit = reddit.subreddit('bitcoin')\n",
    "for post in ml_subreddit.hot(limit=10):\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "posts.to_csv('bitcoin.csv', index=True, encoding='utf-8')\n",
    "print(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bitcoin is the currency of the Internet: a distributed, worldwide, decentralized digital money. Unlike traditional currencies such as dollars, bitcoins are issued and managed without any central authority whatsoever: there is no government, company, or bank in charge of Bitcoin. As such, it is more resistant to wild inflation and corrupt banks. With Bitcoin, you can *be your own bank*.\n",
      "\n",
      "If you are new to Bitcoin, check out [We Use Coins](https://www.weusecoins.com/en/) and [Bitcoin.org](https://www.bitcoin.org/). You can also explore the [Bitcoin Wiki](https://en.bitcoin.it/wiki/Main_Page):\n",
      "\n",
      "* **[Don't invest recklessly](https://www.reddit.com/r/Bitcoin/comments/7gi55s/dont_invest_recklessly/)**  \n",
      "* [Getting Started](https://bitcoin.org/en/getting-started)  \n",
      "* [FAQ / Wiki](https://en.bitcoin.it/wiki/Faq)\n",
      "* [Resources](https://www.lopp.net/bitcoin-information.html)\n",
      "* [Common Myths](https://en.bitcoin.it/wiki/Myths)  \n",
      "* **[How to buy bitcoins worldwide](https://www.buybitcoinworldwide.com/)**  \n",
      "* [Bitcoin as a medium of exchange](https://en.bitcoin.it/wiki/Bitcoin_as_a_medium_of_exchange)\n",
      "* [Will I earn money by mining bitcoin?](https://www.reddit.com/r/Bitcoin/comments/18r5qc/will_i_earn_money_by_mining_an_answer_to_all/)  \n",
      "* [Bitcoin as an investment](https://en.bitcoin.it/wiki/Bitcoin_as_an_investment)\n",
      "* [Storing Bitcoins](https://en.bitcoin.it/wiki/Storing_bitcoins)\n",
      "* [Well-Kept Gardens Die By Pacifism](http://lesswrong.com/lw/c1/wellkept_gardens_die_by_pacifism/)\n",
      "* [A Cypherpunk's Manifesto](https://www.activism.net/cypherpunk/manifesto.html)\n",
      "\n",
      "###Community guidelines\n",
      "\n",
      "* Do not use URL shortening services: always submit the real link.\n",
      "* Begging/asking for bitcoins is absolutely not allowed, no matter how badly you need the bitcoins. Only requests for donations to large, recognized charities are allowed, and only if there is good reason to believe that the person accepting bitcoins on behalf of the charity is trustworthy.\n",
      "* News articles that do not contain the word \"Bitcoin\" are usually off-topic. This subreddit is not about general financial news.\n",
      "* Submissions that are mostly about some other cryptocurrency belong elsewhere. This subreddit is exclusive to Bitcoin.\n",
      "* Promotion of client software which attempts to alter the Bitcoin protocol without overwhelming consensus is not permitted.\n",
      "* No referral links in submissions.\n",
      "* No compilations of free Bitcoin sites.\n",
      "* Trades should usually not be advertised here. For example, submissions like \"Buying 100 BTC\" or \"Selling my computer for bitcoins\" do not belong here. /r/Bitcoin is primarily for news and discussion.\n",
      "* Please avoid repetition — /r/bitcoin is a subreddit devoted to new information and discussion about Bitcoin and its ecosystem. New merchants are welcome to announce their services for Bitcoin, but after those have been announced they are no longer news and should not be re-posted. Aside from *new* merchant announcements, those interested in advertising to our audience should consider [Reddit's self-serve advertising system](https://ads.reddit.com).\n",
      "* Do not post your Bitcoin address unless someone explicitly asks you to.\n",
      "* Be aware that Twitter, etc. is full of impersonation.\n",
      "\n",
      "\n",
      "###Related communities\n",
      "\n",
      "^(Sorted roughly by decreasing popularity.)\n",
      "\n",
      "* **[Bitcoin Beginners](/r/bitcoinbeginners)**\n",
      "* [Local Bitcoin communities](https://www.reddit.com/r/Bitcoin/wiki/local_communities)\n",
      "* [BitcoinMarkets](/r/BitcoinMarkets)\n",
      "* [BitcoinAirdrops](/r/BitcoinAirdrops)\n",
      "* [BitcoinMining](/r/bitcoinmining)\n",
      "* [BitcoinTaxes](/r/bitcointaxes) [[CryptoTax](/r/CryptoTax)]\n",
      "* [BitMarket](/r/BitMarket)\n",
      "* [Jobs4Bitcoin] (/r/Jobs4bitcoins)\n",
      "* [Girls Gone Bitcoin](/r/GirlsGoneBitcoin) (NSFW)\n",
      "* [CryptoMarkets] (/r/CryptoMarkets)\n",
      "* [BitcoinDiscussion](/r/BitcoinDiscussion)\n",
      "* [BitcoinTechnology](/r/BitcoinTechnology)\n",
      "* [Best of Crypto] (/r/Best_of_Crypto)\n",
      "* [[More]] (https://www.reddit.com/r/Bitcoin/wiki/communities)\n",
      "\n",
      "^(Non-Bitcoin communities)\n",
      "\n",
      "* [Technology](/r/technology)\n",
      "* [Economics](/r/economics)\n",
      "* [Crypto](/r/crypto)\n",
      "* [Anarcho-Capitalism](/r/Anarcho_Capitalism)\n",
      "* [CryptoCurrencies](/r/CryptoCurrencies)\n",
      "* [CryptoAnarchy](/r/cryptoanarchy)\n",
      "* [[More]](https://www.reddit.com/r/Bitcoin/wiki/communities)\n",
      "\n",
      "###Join us on IRC\n",
      "\n",
      "[web.libera.chat *#bitcoin*](irc://irc.libera.chat/bitcoin)\n",
      "\n",
      "###Other Bitcoin sites\n",
      "\n",
      "**[Bitcoin Forum](https://bitcointalk.org)**  \n",
      "[Bitcoin Stack Exchange](https://bitcoin.stackexchange.com/)  \n",
      "[Bitcoin Magazine](https://bitcoinmagazine.com/)\n",
      "\n",
      "###Download Bitcoin Core\n",
      "\n",
      "[**Bitcoin Core**](https://bitcoincore.org/en/blog/) is the [backbone of the Bitcoin network](https://en.bitcoin.it/wiki/Full_node). Almost all Bitcoin wallets rely on Bitcoin Core in one way or another. If you have a fairly powerful computer that is almost always online, you can help the network by running Bitcoin Core. You can also use Bitcoin Core as a very secure Bitcoin wallet.\n",
      "\n",
      "* Latest stable version: [***23.0 (April 2022)***](https://bitcoincore.org/en/download/) \\[[BitTorrent](https://www.reddit.com/r/Bitcoin/wiki/bitcoin_core_bittorrent)\\]\n",
      "* [Release Announcement](https://github.com/bitcoin/bitcoin/blob/master/doc/release-notes/release-notes-23.0.md)\n",
      "* You *MUST* [verify the integrity of this software](/r/Bitcoin/wiki/verifying_bitcoin_core) before running it.\n",
      "\n",
      "**Style sheet credits**\n",
      "\n",
      "The CSS used by this subreddit is the Erdune Theme modified by /u/Annihilia and /u/konkedas. Logo design by /u/Annihilia. Check out his other work [here](https://www.lunarcannon.com/).\n"
     ]
    }
   ],
   "source": [
    "print(ml_subreddit.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4m_mtbgY5Hq"
   },
   "source": [
    "# Top Submissions in the Past Year:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2256,
     "status": "ok",
     "timestamp": 1614485579193,
     "user": {
      "displayName": "BitsInBytes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj4_qtDugwTI7x9UjhXAbLPMQvtcyQD3UMj8gLc=s64",
      "userId": "13908254166643482828"
     },
     "user_tz": 360
    },
    "id": "w2frnq1OczG3",
    "outputId": "18bc7403-daad-472e-c267-0cff6acdc284",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\232639795.py:1: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  subs = reddit.subreddit('bangladesh').top('year')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At we came up with our own..! Right? , 86 , 707\n",
      "We Bangladeshis are simply made differently. , 61 , 682\n",
      "We started with 3 to 5 people. Thanks to r/Bangladesh community, we were able to keep our flag on the Canvas. I would also like to thank the mod for helping us out and listening to the National Anthem with the community was a powerful feeling inside. Thank you \"The Daily Star\" covering this story! , 65 , 555\n",
      "His name is Omar Sharif. His bookstore is on the opposite of the Rangpur City Corporation, at Shaheed Jorrez Market. He is 60 years old. He sells books at nominal prices, and rents books for 5 BDT, 2 BDT. When someone reads the book again and says he likes it, he returns the whole money. , 52 , 530\n",
      "A proud moment for us. , 57 , 524\n",
      "Dunno the Bengali meme sub so hopefully this fits in here? , 109 , 470\n",
      "girl just got yeeted by a journalist , 56 , 470\n",
      "Bengal famine of 1943: our people paid the price for even WW2. God bless the queen! Long live the empire! , 68 , 428\n",
      "Deshi Transition , 28 , 408\n",
      "Happy 50th Bandhu 🤝🏾 , 40 , 395\n",
      "humanity for all , 33 , 384\n",
      "DHAKA's SKY on NYE 2022 , 39 , 379\n",
      "AI generated image of the word \"bangladesh\" , 29 , 383\n",
      "The Majestic Padma And The Newly Made Bridge. 📸 Mahfujul Hasan , 41 , 370\n",
      "Perfection , 25 , 366\n",
      "Time lapse of Bangladesh, Singapore and Malaysia! , 62 , 361\n",
      "আমরা বাংলাদেশি😁😁 , 64 , 364\n",
      "My 15yr old cousin drew the Royal Bengal Tiger, That loser has 0 confidence and self worth in himself and that makes me want to beat him with a ঝারু until he decides to respect himself for the person he is. But overall, do give feedback on how the drawing is, he worked 16 hours straight on this! , 39 , 356\n",
      "Bangladesh Slanders , 62 , 354\n",
      "WoW! , 50 , 352\n",
      "Average Bangladeshi in a nutshell , 170 , 356\n",
      "Why does this slap lmao..... , 30 , 349\n",
      "Bongobondhu Sheikh Mujibur Rahman tunnel, the first underwater tunnel in South Asia 🇧🇩 , 142 , 337\n",
      "Bangladesh 1971. A mother stands guard over her children. , 15 , 328\n",
      "Things you have to do to protect your land in BD , 22 , 322\n",
      "দয়ার সাগর বাংলার পুলিশ , 52 , 313\n",
      "No one appreciates how smart the symbol for taka (৳) is , 50 , 311\n",
      "Police Officer Rescues Puppy From Busy Road ! , 15 , 311\n",
      "Sick of being a female in this country , 120 , 309\n",
      "Only in Bangladesh 💀🤣 , 69 , 302\n",
      "Haha , 96 , 306\n",
      "I made a low poly dhaka city with metro rail scene in blender. 👨‍🎨 , 51 , 294\n",
      "Pakistani Author, Anam Zakaria, on how Racial Stereotypes Caused Pakistan to Underestimate the Intensity of Bengali Militant Resistance During the Independence War , 80 , 295\n",
      "Body painting by Bangladeshi 🇧🇩 Artist. Human canvas- Ms shayla azad. Artist- Anjum Solaiman. , 116 , 293\n",
      "Shall tje truth be spoken , 23 , 291\n",
      "Yeah I slept and this happened. Definitely not surviving to see tomorrow , 65 , 287\n",
      "Final Art at r/place before the White Void , 39 , 285\n",
      "আন্তর্জাতিক পেশাদার বক্সিং-এ বাংলাদেশের প্রথম স্বর্ণপদক জয় | সুরো কৃষ্ণ চাকমা , 56 , 283\n",
      "This is made only by using powdered rice and own hands. , 23 , 284\n",
      "TikToker arrested for unscrewing nuts, bolts of Padma Bridge , 92 , 284\n",
      "Drew Behula in My style. , 52 , 276\n",
      "Avik Anwar becomes the first Bangladeshi racer to win on a Formula1 track | Source: The Business Standard | Image: Collected , 32 , 274\n",
      "Role of American racism in Bengali genocide. , 85 , 272\n",
      "Police officer taking bribe openly on the road .If not given the ifficer will stop the vehicle and file a case ! , 38 , 266\n",
      "I made the flag of Bangladesh in minecraft pocket edition , 25 , 268\n",
      "A Bengali labor selling fruits in his part time in UAE. , 16 , 260\n",
      "Power of Bangladeshi People Hand 😄 , 15 , 263\n",
      "The lack of action by police represents the role of government , 135 , 257\n",
      "3 countries in one frame , 26 , 255\n",
      "State of South Asia rn , 133 , 254\n",
      "Freedom fighters for the newly independent state of Bangladesh surround Razakars, members of a paramilitary volunteer force supported by the Pakistani military regime, circa 1971. , 98 , 254\n",
      "privacy where? , 122 , 254\n",
      "Somewhere at the University of Chittagong | close to 22°28'27.2\"N 91°47'18.9\"E , 23 , 250\n",
      "Drone Show Near Hatirjheel, in Celebration of Bangladesh’s Golden Jubilee of Independence and Development, depicting crucial projects and history of Bangladesh , 86 , 249\n",
      "No title , 98 , 245\n",
      "জীবনের গিয়ার শিফট করে দিলো , 42 , 244\n",
      "Newborns d*i *e after hospital authorities stop oxygen supply for bill payments , 108 , 243\n",
      "Bruh , 28 , 241\n",
      "Are we surprised? , 138 , 235\n",
      "What a terrible internet network Teletalk is! , 36 , 237\n",
      "Prices taking another stonk towards the moon! Credit: Asif Zaman (facebook) , 18 , 235\n",
      "2022 Sitakunda fire: Killing 49 and injuring around 450+. Plus keep them in your preyar. , 40 , 234\n",
      "This guy made fun of us because we say \"Cigarette khai\" i.e \"Eating cigaratte. What an idiot he is. , 54 , 238\n",
      "Winter morning, Village ♥️ my own click 📸 15/11/21 device: Redmi note 7s , 21 , 231\n",
      "সুন্দরবনে তেলিয়া ভোলা প্রজাতির। এই মাছ ধরা পরে মাছটির ওজন ৭৮ কেজি ৪০০ গ্রাম। এবং সেটা ৪৩ লাখ টাকায় বিক্রি হয়। , 37 , 231\n",
      "Once upon a time there were thousand's of bengal tiger in bangladesh, but now there's only around 100 and i don't think so that our government is doing something about it. So we can say in some years tiger's will be extinct from Bangladesh, after that what will be our national animal. , 85 , 229\n",
      "Location: Homna, Comilla , 21 , 227\n",
      "Shahbag,2013 , 95 , 229\n",
      "I'm glad that they realize now , 125 , 228\n",
      "কুমিল্লা এয়ারস্ট্রিপ ১৯৪৪ সালে দ্বিতীয় বিশ্বযুদ্ধের সময়। ছবি সূত্র : CBI WWII , 26 , 227\n",
      "We are heading towards a huge crisis for which we did not prepare at all! , 59 , 226\n",
      "Notable Inventions by Our Eternal Leader Bongobondhu Sheikh Mujibur Rahman (Disclaimer: claim verification pending) , 36 , 223\n",
      "Still waiting , 22 , 221\n",
      "boys played well, edition 1971 , 13 , 221\n",
      "Female students peacefully protesting totalitarian Pakistani establishment. 1970 pre war. [colorized with AI] , 24 , 221\n",
      "Shocked to know about this bangladeshi superhero in Marvel.. Her name is Enigma. Real name is Tara Virango. I am suprised nobody knows about this. , 42 , 219\n",
      "In thakurgaon upazila, other restaurant owners are increasing food prices because of oil price hike. But a small restaurant owner Abdul Hamid refuses to increase food prices and is using soyabin oil as saline system to cook. He is saving now 1 liter oil per day while frying 250 parathas as before. , 26 , 222\n",
      "The primary harasser of Narshindi Railway Station incident has finally been detained. A few months in jail should teach her a lesson , 123 , 218\n",
      "Happy new year!!! , 12 , 219\n",
      "Eid er suvassa. Eid Mobarock , 49 , 219\n",
      "A battle to have some food - Sylhet , 56 , 212\n",
      "Happy Birthday Bangladesh , 43 , 211\n",
      "India's recognition of Bangladesh, 6 December 1971 🇧🇩 , 154 , 211\n",
      "দুটো টাকার-ই বয়স ৭ বছর কিন্তু পার্থক্য হলো ওপরেরটি এই পৃথিবীর রূপ দেখেনি আর নিচেরটি এই পৃথিবীর রুপ দেখেছে। photo Collected.. , 37 , 208\n",
      "Dhaka in...1954.. Photo Collected , 55 , 207\n",
      "Literally Every News Channel Right now , 51 , 208\n",
      "Tourist Police Logic , 18 , 209\n",
      "The topography of Bangladesh. , 34 , 207\n",
      "Test-run of Dhaka Metro at Uttara Central station, this time at the speed of 100km/h. The Line 6 will start its commercial operation in December in 2022. (Video was taken in November 2021) , 31 , 203\n",
      "স্বপ্নের পদ্মা সেতু এখন বাস্তবে পরিণত। আর এক ধাপ এগিয়ে সোনার বাংলাদেশ। , 55 , 203\n",
      "Nilkomol ar Lalkomol, me, digital, 2022 [OC] , 35 , 200\n",
      "নতুন অতিথি , 29 , 203\n",
      "খুব গর্ব হচ্ছে 🙂 , 43 , 200\n",
      "First country in South Asia to open e-gates for e-passports , 71 , 200\n",
      "Despite all rhetoric about Bangladesh leaning towards India or China, this graph accurately captures who have actually helped Bangladesh. , 46 , 197\n",
      "Village Bangla, Sirajganj , 13 , 199\n",
      "Tiler khaja - a crunchy sweet treat from Kushtia filled with sesame , 21 , 195\n",
      "[OC] Beautiful Bangladesh | Niladri Lake, Sunamganj , 8 , 197\n",
      "What a win by Bangladesh! They beat the world champs! , 32 , 200\n",
      "I made a flag of Bangladesh. , 22 , 198\n"
     ]
    }
   ],
   "source": [
    "subs = reddit.subreddit('bangladesh').top('year')\n",
    "\n",
    "for submission in subs:\n",
    "    print(submission.title,',',submission.num_comments,',',submission.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 2368,
     "status": "ok",
     "timestamp": 1614485586806,
     "user": {
      "displayName": "BitsInBytes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj4_qtDugwTI7x9UjhXAbLPMQvtcyQD3UMj8gLc=s64",
      "userId": "13908254166643482828"
     },
     "user_tz": 360
    },
    "id": "_cdXp4MPEeAW",
    "outputId": "2e10713a-6d0c-4620-a922-a76f640b35b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data siens , 75 , 3138\n",
      "Shout Out to All the Mediocre Data Scientists Out There , 260 , 2930\n",
      "It’s never too early , 59 , 2908\n",
      "Graph of graph analysis , 42 , 2257\n",
      "The next time my coworkers ask what metrics I used for my model. , 66 , 1876\n",
      "Fit an exponential curve to anything... , 87 , 1859\n",
      "It's Meme Monday, so here's a python meme for DS folks , 86 , 1838\n",
      "I created a four-page Data Science Cheatsheet to assist with exam reviews, interview prep, and anything in-between , 88 , 1828\n",
      "Data Engineering , 47 , 1754\n",
      "A \"Data Science\" company stole my gf's ML project and reposted it as their own. What do I do? , 77 , 1466\n",
      "I created a complete overview of machine learning concepts seen in 27 data science and machine learning interviews , 104 , 1240\n",
      "Distributed Computing and SQL , 61 , 1050\n",
      "Useless tutorials and blog post will NOT improve your CV but WILL waste our time , 145 , 1047\n",
      "I got a job!! , 108 , 1027\n",
      "To All \"Data Scientists\" out there, Crowdsourcing COVID-19 , 181 , 975\n",
      "20 Best Libraries for Data Science in R , 87 , 970\n",
      "Remember to stop every once in a while and think about how far you've come. , 54 , 888\n",
      "Landing a Senior Data Scientist Job After 6 Months of Unemployment , 117 , 871\n",
      "Please STOP asking Data Scientists about Leetcode questions meant for Software Engineers for job interviews , 344 , 868\n",
      "Experience/Advice from a 10+ year data scientist , 86 , 855\n",
      "I'm sick of \"AI Influencers\" - especially ones that parade around with a bunch of buzzwords they don't understand! , 345 , 843\n",
      "My Giant Data Quality Checklist , 58 , 818\n",
      "It seems a lot of people want to get into the data science field without having the slightest idea of what it actually entails , 222 , 816\n",
      "Data Scientist = Web Master from the 90s , 75 , 807\n",
      "I got the internship!!! , 117 , 774\n",
      "Scientists rename human genes to stop Microsoft Excel from misreading them as dates - The Verge , 188 , 766\n",
      "Beginner project for SQL. This is a simple python script to scrape stock prices off NASDAQ API and feed it to MySQL. , 59 , 766\n",
      "What we look for in hiring , 145 , 758\n",
      "'A scary time': Researchers react to agents raiding home of former Florida COVID-19 data scientist , 259 , 741\n",
      "Is anybody else here trying to actively push back against the data science hype? , 301 , 750\n",
      "I'm a Senior Data Scientist at Disney and I'm hosting another free Data Science Q&A session this Thursday @ 5:30 PM PST , 29 , 740\n",
      "My first technical interview experience(22+ interview questions) , 113 , 739\n",
      "Amazon's Machine Learning University is making its online courses available to the public , 41 , 722\n",
      "If you needed yet another reason to convince you that Excel is terrible for data science... , 152 , 709\n",
      "Haiti had its first data science bootcamp , 8 , 711\n",
      "We're data scientists planning a virtual career fair for other data pros during COVID-19. Looking for a job? Looking for help? , 115 , 707\n",
      "Unethical Nobel Behaviour , 67 , 709\n",
      "[Source code with demo] Here is my python implementation of Deep Q-learning for playing Tetris , 56 , 705\n",
      "A little advice after 15 years in this field as an industry practitioner and academic. , 165 , 693\n",
      "Landed my first full time job today - Data Engineering , 191 , 697\n",
      "So many people disappointed with their jobs. You need to manage your expectations, especially if you're very junior. , 91 , 685\n",
      "We Need More Data Engineers, Not Data Scientists , 186 , 675\n",
      "If anyone is really into keyboard shortcuts like I am I just found a guide that has a ton of them for many IDE's. Includes: Python, Tableu, Excel, SQL, R, SAS, SPSS, Matlab & Stata. , 37 , 677\n",
      "I am tired of being assessed as a 'software engineer' in job interviews. , 194 , 656\n",
      "Comprehensive Python Cheatsheet now also covers Pandas , 32 , 653\n",
      "I'm a Senior Data Scientist at Disney and I'm hosting another Data Science Q&A session this Thursday @ 5:30 PM PST. I'll be joined by a Principal Data Scientist at Clearbanc! , 39 , 655\n",
      "Why the ability to concentrate is the most important skill in 2020 , 107 , 656\n",
      "First Year As A Data Scientist Reflection , 45 , 649\n",
      "Elon Musk has said he will demonstrate a functional brain-computer interface this week during a live presentation from his mysterious Neuralink startup. , 105 , 639\n",
      "Today I reached a new milestone: got rejected from an internship in 5 hours! , 111 , 635\n",
      "Keep a Brag Document , 30 , 637\n",
      "What makes a good personal project - from the perspective of a hiring manager , 57 , 635\n",
      "You probably should be using JupyterLab instead of Jupyter Notebooks , 198 , 632\n",
      "Udacity is offering access to their courses for free due to COVID-19 , 117 , 618\n",
      "/r/datascience enters TOP 1000 subreddits , 27 , 609\n",
      "Data Science job requirements these days are so ridiculous that even reading them boils my blood. , 145 , 605\n",
      "IAMA Senior Data Scientist at Disney and I’m setting up free Q&A sessions to help people who are looking to enter/transition into data science , 71 , 599\n",
      "XKCD : Confidence Interval , 26 , 599\n",
      "Off My Chest. After almost 9+ face-to-face interviews I cannot even get a simple entry level Data Analyst job for almost 7 months now. Any advice? , 90 , 589\n",
      "How many of you are hybrids of data analyst, data scientist, and data engineer? , 141 , 598\n",
      "You've just been given a dataset with 500k records and 50+ columns to build a predictive model by the end of the day. What mental checklist do you go through to build a model as quickly and accurately as possible? , 201 , 596\n",
      "I find this data science map really useful. Where are you on it? , 99 , 590\n",
      "Would anyone be interested in a “soft data science” series? , 98 , 575\n",
      "Pandas is so cool , 199 , 577\n",
      "Creating a discord channel for those interested in becoming a data analyst. Will do weekly data visualisation projects with peer to peer code reviews. , 94 , 570\n",
      "All Cambridge University textbooks are free in HTML format until the end of May , 78 , 564\n",
      "I got the job!! , 40 , 561\n",
      "Tired of Siraj Raval's plagiarism? Here's what you can do , 61 , 554\n",
      "Stop giving extra tests, filling out lengthy applications, just to throw 80% of it in the trash when the optimal candidate arises [RANT] , 279 , 548\n",
      "Rant: Don't put bachelors as a minimum if you only hire masters. , 172 , 537\n",
      "Today is R's 20th birthday. Here is how much bigger, stronger and faster it got over the years - Jozef's Rblog , 16 , 528\n",
      "Does anyone get annoyed when people say “AI will take over the world”? , 344 , 525\n",
      "Absolutely failed a data science pre-screening test, huge wake up call for me , 106 , 518\n",
      "Rookie Data Science Mistake Invalidates a Dozen Medical Studies , 50 , 523\n",
      "What hiring managers are really looking for , 112 , 512\n",
      "I’ve made this LIVE Interactive dashboard to track COVID19, any suggestions are welcome , 37 , 500\n",
      "What is up with this subreddit. A plea for help , 117 , 499\n",
      "Data science job market shrinking while data engineering is exploding , 140 , 495\n",
      "100-days Data Science Challenge! , 67 , 485\n",
      "Free Mathematics Courses for Data Science & Machine Learning , 21 , 484\n",
      "\"Hyperparameter Optimisation\" is the ultimate cheat code to buy your ML project more time. , 65 , 484\n",
      "Networking - it's not just LinkedIn connections , 75 , 472\n",
      "Data Science Podcasts , 66 , 467\n",
      "Numpy , 154 , 469\n",
      "To all the data scientists with ADD, here are some tips to help! , 45 , 465\n",
      "GridSearchCV 2.0 - Up to 10x faster than sklearn , 61 , 456\n",
      "Increase in low quality Medium articles? , 111 , 451\n",
      "Angry rant , 223 , 450\n",
      "SAS is easily one of the worst languages I have ever had to learn , 157 , 451\n",
      "Running a script? Creating an extract? Get your ass out of your chair and stretch , 41 , 449\n",
      "Illustrated Data Science study guides covering MIT’s 15.003 class , 18 , 445\n",
      "How hard data science actually is? , 190 , 449\n",
      "Towards Data science articles quality are degrading , 126 , 446\n",
      "Floods of junior applicants are forcing companies to erase Data Scientist positions for Senior ones , 266 , 444\n",
      "Seriously, how am I expected to grow in a profession where everyone discourages me from building anything non-trivial , 147 , 447\n",
      "What is the most interesting public dataset you know of? , 89 , 439\n",
      "I got the chance to interview a Data Scientist at Uber on their Shared Rides Team! , 39 , 442\n",
      "Any Employed Data Scientists Willing to Share an Average Day at Work? , 57 , 438\n",
      "My Apologies - From \"A Data Science company stole my gf's ML project and reposted it as their own. What do I do?\" , 125 , 422\n",
      "[MEME] The hierarchy of data science , 86 , 422\n"
     ]
    }
   ],
   "source": [
    "for submission in reddit.subreddit(\"datascience\").top('year'):\n",
    "    print(submission.title,',',submission.num_comments,',',submission.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JzvaR0XNY-5-"
   },
   "source": [
    "# Get Data into a Pandas Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9268,
     "status": "ok",
     "timestamp": 1612634715772,
     "user": {
      "displayName": "BitsInBytes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj4_qtDugwTI7x9UjhXAbLPMQvtcyQD3UMj8gLc=s64",
      "userId": "13908254166643482828"
     },
     "user_tz": 360
    },
    "id": "5mGcfS_KG3ZQ",
    "outputId": "742935ae-1326-4bac-b63f-be958a12d415"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n",
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Official] 2020 End of Year Salary Sharing thread</td>\n",
       "      <td>110</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/klvb55/official_2020_end_of_year_salary_sharing_thread/</td>\n",
       "      <td>237</td>\n",
       "      <td>See [last year's Salary Sharing thread here](https://www.reddit.com/r/datascience/comments/e8fown/official_2019_end_of_year_salary_sharing_thread/).\\n\\n**MODNOTE**: Borrowed this from [r/cscareerquestions](https://www.reddit.com/r/cscareerquestions/). Some people like these kinds of threads, some people hate them. If you hate them, that's fine, but please don't get in the way of the people who find them useful. Thanks!\\n\\nThis is the official thread for sharing your current salaries (or recent offers).\\n\\nPlease only post salaries/offers if you're including hard numbers, but feel free to use a throwaway account if you're concerned about anonymity. You can also generalize some of your answers (e.g. \"Large biotech company\"), or add fields if you feel something is particularly relevant.\\n\\n* **Title:**\\n* **Tenure length:**\\n* **Location:**\\n* **Salary:**\\n* **Company/Industry:**\\n* **Education:**\\n* **Prior Experience:**\\n   * **$Internship**\\n   * **$Coop**\\n* **Relocation/Signing Bonus:**\\n* **Stock and/or recurring bonuses:**\\n* **Total comp:**\\n\\nNote that while the primary purpose of these threads is obviously to share compensation info, discussion is also encouraged.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Weekly Entering &amp; Transitioning Thread | 31 Jan 2021 - 07 Feb 2021</td>\n",
       "      <td>6</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/l9b00a/weekly_entering_transitioning_thread_31_jan_2021/</td>\n",
       "      <td>116</td>\n",
       "      <td>Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\\n\\n* Learning resources (e.g. books, tutorials, videos)\\n* Traditional education (e.g. schools, degrees, electives)\\n* Alternative education (e.g. online courses, bootcamps)\\n* Job search questions (e.g. resumes, applying, career prospects)\\n* Elementary questions (e.g. where to start, what next)\\n\\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is anybody else here trying to actively push back against the data science hype?</td>\n",
       "      <td>318</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/ldvl72/is_anybody_else_here_trying_to_actively_push_back/</td>\n",
       "      <td>129</td>\n",
       "      <td>So I'd expected the hype to die off by now, but if anything it's getting worse. Are there any groups out there actively pushing back against the ridiculous hype?\\n\\nI've worked as a data scientist for 5+ years now, and have recently been looking for a new position. I'm honestly shocked at how some of the interviewers seem to view a data science job as little more than an extended Kaggle competition.\\n\\nA few days ago, during an interview, I was told \"We want to build a neural network\" - I've started really pushing back in interviews. My response was along the lines: you don't need a neural network, Jesus you don't have any infrastructure and your data is beyond shite (all said politely in a non-condescending way, just paraphrasing here!).\\n\\nI went on to talk about the value they CAN get out of ML and how we could build up to NN. I laid out a road map: Let's identify what problems your business is trying to solve (hint might not even need ML), eventually scope and translate those business problems into ML projects, start identifying ways in which we can improve your data quality, start building up some infrastructure, and for the love of god start automating processes because clearly I will not be processing all your data by hand. Update: Some people seem to think I did this in a rude way: guys I was professional at all times. I'm paraphrasing with a little dramatic flair - don't take it verbatim.\\n\\nTo my surprise, people gloss over at this point. They really were not interested in hearing about how one would go about project managing large data science problems. Or hearing about my experience in DS project management. They just wanted to hear buss words and know whether I knew particular syntax. They were even more baffled when I told them I have to look up half the syntax, because I automate most of the low-level stuff - as I'm sure most of us do. There seems to be such a disconnect here. It just baffles me. Employers seem to have quite a warped view of day-to-day life as a data scientist.\\n\\nSo is anybody else here trying to push back against the data science hype at work etc? If so, how? And if many of us are doing this then why is the hype not dialling back? Why have companies not matured.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why did Python become one of the languages of choice for data science?</td>\n",
       "      <td>318</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/ldchgb/why_did_python_become_one_of_the_languages_of/</td>\n",
       "      <td>162</td>\n",
       "      <td>Obviously at a certain point, a language’s specialization becomes a feedback loop; everyone makes libraries for those things for that language because there’s already so much available support. \\n\\nThat said, how did everyone come to settle on Python before it reached that level of saturation?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How different is working as a data scientist different from schooling? How prepared am I for the work force?</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/le1tld/how_different_is_working_as_a_data_scientist/</td>\n",
       "      <td>7</td>\n",
       "      <td>Some background on me. I am currently working on a M.S. in applied statistics, looking to graduate in December. I have a bachelors in mathematics I completed a little over a year ago. I have an internship set up for this summer.\\n\\nHowever, I am nervous that I am still unprepared to work as a data scientist full time. I have no frame of reference because I haven’t worked in industry or completed an internship yet. I believe the closest class I’ve taken to real world was Data Mining, where we were given a data set and had to clean it and use ML techniques to find an accurate prediction rate. \\n\\nDoes this translate to day to day as a data scientist? Should I be doing Kaggle competitions to better prepare myself for industry? Any tips for someone like me?\\n\\nThank you Reddit community!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>Data Science Help! Data Transparency Tools!</td>\n",
       "      <td>1</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/jvkdcv/data_science_help_data_transparency_tools/</td>\n",
       "      <td>13</td>\n",
       "      <td>Hey everyone! I am working on data transparency... Do any of you have experience with explanation AI? I wondering what has worked for small teams in dealing with data handling regulations like GDPR and CCPA. Thank you so much!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>Definitions for a dashboard</td>\n",
       "      <td>8</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/jv6r90/definitions_for_a_dashboard/</td>\n",
       "      <td>9</td>\n",
       "      <td>Hey,\\n\\nWe are currently using a Qlikview dashboard with KPIs etc. My boss is now asking me to create and extensive \"definition handbook\" in which every piece of data used in the dashboard is defined and explained. Such as: which data is used for year-related filtered (start / closing year etc). Is this standard procedure when developing a dashboard? Or is this information usually included on the dasboard itself? I find myself wondering where to start and what to define. Google didn't help me either, would love to see some examples of definition handbooks/ guidelines. \\n\\n&amp;#x200B;\\n\\nThanks for the help.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Angry rant</td>\n",
       "      <td>444</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/ju6uox/angry_rant/</td>\n",
       "      <td>223</td>\n",
       "      <td>I’m tired of sending out job applications to entry level jobs and being snuffed out by people with senior level experience and phds \\n\\nI’m tired of filling out a whole ass job login page, Re write my entire resume onto their shit tier career account, and then hear nothing back, OR take an assessment thus spending an hour for nothing. \\n\\nI’m tired of companies that do call me back offer shit money when it’s clear that I’m worth average market value. \\n\\nI’m tired of complaining to friends, family, girlfriend, and the internet. \\n\\nI’m tired of recruiters saying “yeah man it’s a bad market” \\n\\nthanks COVID. I hate 2020.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>Weekly Entering &amp; Transitioning Thread | 15 Nov 2020 - 22 Nov 2020</td>\n",
       "      <td>7</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/juku7d/weekly_entering_transitioning_thread_15_nov_2020/</td>\n",
       "      <td>159</td>\n",
       "      <td>Welcome to this week's entering &amp; transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\\n\\n* Learning resources (e.g. books, tutorials, videos)\\n* Traditional education (e.g. schools, degrees, electives)\\n* Alternative education (e.g. online courses, bootcamps)\\n* Job search questions (e.g. resumes, applying, career prospects)\\n* Elementary questions (e.g. where to start, what next)\\n\\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&amp;restrict_sr=1&amp;sort=new).</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>Is there any value in understanding and applying the underlying theory?</td>\n",
       "      <td>3</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/jujktx/is_there_any_value_in_understanding_and_applying/</td>\n",
       "      <td>12</td>\n",
       "      <td>So to expand, I have a project idea of doing a fairly simple linear regression, but not in 3 lines of code.\\n\\nI'm interested to see if I can use my academic understanding to do this. so going from the data to minimising my SSR to then solving this system of equations using the OLS formula of (X'X)^{-1}X'Y1 to obtain my OLS estimate. \\n\\nIn industry it seems redundant but as a side project would this be valuable or make me seem a little more engaged.\\n\\nI am just trying to gauge whether it is s good use of my time.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>507 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                            title  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        body\n",
       "0                                                               [Official] 2020 End of Year Salary Sharing thread  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       See [last year's Salary Sharing thread here](https://www.reddit.com/r/datascience/comments/e8fown/official_2019_end_of_year_salary_sharing_thread/).\\n\\n**MODNOTE**: Borrowed this from [r/cscareerquestions](https://www.reddit.com/r/cscareerquestions/). Some people like these kinds of threads, some people hate them. If you hate them, that's fine, but please don't get in the way of the people who find them useful. Thanks!\\n\\nThis is the official thread for sharing your current salaries (or recent offers).\\n\\nPlease only post salaries/offers if you're including hard numbers, but feel free to use a throwaway account if you're concerned about anonymity. You can also generalize some of your answers (e.g. \"Large biotech company\"), or add fields if you feel something is particularly relevant.\\n\\n* **Title:**\\n* **Tenure length:**\\n* **Location:**\\n* **Salary:**\\n* **Company/Industry:**\\n* **Education:**\\n* **Prior Experience:**\\n   * **$Internship**\\n   * **$Coop**\\n* **Relocation/Signing Bonus:**\\n* **Stock and/or recurring bonuses:**\\n* **Total comp:**\\n\\nNote that while the primary purpose of these threads is obviously to share compensation info, discussion is also encouraged.\n",
       "1                                              Weekly Entering & Transitioning Thread | 31 Jan 2021 - 07 Feb 2021  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\\n\\n* Learning resources (e.g. books, tutorials, videos)\\n* Traditional education (e.g. schools, degrees, electives)\\n* Alternative education (e.g. online courses, bootcamps)\\n* Job search questions (e.g. resumes, applying, career prospects)\\n* Elementary questions (e.g. where to start, what next)\\n\\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).\n",
       "2                                Is anybody else here trying to actively push back against the data science hype?  ...  So I'd expected the hype to die off by now, but if anything it's getting worse. Are there any groups out there actively pushing back against the ridiculous hype?\\n\\nI've worked as a data scientist for 5+ years now, and have recently been looking for a new position. I'm honestly shocked at how some of the interviewers seem to view a data science job as little more than an extended Kaggle competition.\\n\\nA few days ago, during an interview, I was told \"We want to build a neural network\" - I've started really pushing back in interviews. My response was along the lines: you don't need a neural network, Jesus you don't have any infrastructure and your data is beyond shite (all said politely in a non-condescending way, just paraphrasing here!).\\n\\nI went on to talk about the value they CAN get out of ML and how we could build up to NN. I laid out a road map: Let's identify what problems your business is trying to solve (hint might not even need ML), eventually scope and translate those business problems into ML projects, start identifying ways in which we can improve your data quality, start building up some infrastructure, and for the love of god start automating processes because clearly I will not be processing all your data by hand. Update: Some people seem to think I did this in a rude way: guys I was professional at all times. I'm paraphrasing with a little dramatic flair - don't take it verbatim.\\n\\nTo my surprise, people gloss over at this point. They really were not interested in hearing about how one would go about project managing large data science problems. Or hearing about my experience in DS project management. They just wanted to hear buss words and know whether I knew particular syntax. They were even more baffled when I told them I have to look up half the syntax, because I automate most of the low-level stuff - as I'm sure most of us do. There seems to be such a disconnect here. It just baffles me. Employers seem to have quite a warped view of day-to-day life as a data scientist.\\n\\nSo is anybody else here trying to push back against the data science hype at work etc? If so, how? And if many of us are doing this then why is the hype not dialling back? Why have companies not matured.\n",
       "3                                          Why did Python become one of the languages of choice for data science?  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Obviously at a certain point, a language’s specialization becomes a feedback loop; everyone makes libraries for those things for that language because there’s already so much available support. \\n\\nThat said, how did everyone come to settle on Python before it reached that level of saturation?\n",
       "4    How different is working as a data scientist different from schooling? How prepared am I for the work force?  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Some background on me. I am currently working on a M.S. in applied statistics, looking to graduate in December. I have a bachelors in mathematics I completed a little over a year ago. I have an internship set up for this summer.\\n\\nHowever, I am nervous that I am still unprepared to work as a data scientist full time. I have no frame of reference because I haven’t worked in industry or completed an internship yet. I believe the closest class I’ve taken to real world was Data Mining, where we were given a data set and had to clean it and use ML techniques to find an accurate prediction rate. \\n\\nDoes this translate to day to day as a data scientist? Should I be doing Kaggle competitions to better prepare myself for industry? Any tips for someone like me?\\n\\nThank you Reddit community!\n",
       "..                                                                                                            ...  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         ...\n",
       "502                                                                   Data Science Help! Data Transparency Tools!  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Hey everyone! I am working on data transparency... Do any of you have experience with explanation AI? I wondering what has worked for small teams in dealing with data handling regulations like GDPR and CCPA. Thank you so much!!\n",
       "503                                                                                   Definitions for a dashboard  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Hey,\\n\\nWe are currently using a Qlikview dashboard with KPIs etc. My boss is now asking me to create and extensive \"definition handbook\" in which every piece of data used in the dashboard is defined and explained. Such as: which data is used for year-related filtered (start / closing year etc). Is this standard procedure when developing a dashboard? Or is this information usually included on the dasboard itself? I find myself wondering where to start and what to define. Google didn't help me either, would love to see some examples of definition handbooks/ guidelines. \\n\\n&#x200B;\\n\\nThanks for the help.\n",
       "504                                                                                                    Angry rant  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        I’m tired of sending out job applications to entry level jobs and being snuffed out by people with senior level experience and phds \\n\\nI’m tired of filling out a whole ass job login page, Re write my entire resume onto their shit tier career account, and then hear nothing back, OR take an assessment thus spending an hour for nothing. \\n\\nI’m tired of companies that do call me back offer shit money when it’s clear that I’m worth average market value. \\n\\nI’m tired of complaining to friends, family, girlfriend, and the internet. \\n\\nI’m tired of recruiters saying “yeah man it’s a bad market” \\n\\nthanks COVID. I hate 2020.\n",
       "505                                            Weekly Entering & Transitioning Thread | 15 Nov 2020 - 22 Nov 2020  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Welcome to this week's entering & transitioning thread! This thread is for any questions about getting started, studying, or transitioning into the data science field. Topics include:\\n\\n* Learning resources (e.g. books, tutorials, videos)\\n* Traditional education (e.g. schools, degrees, electives)\\n* Alternative education (e.g. online courses, bootcamps)\\n* Job search questions (e.g. resumes, applying, career prospects)\\n* Elementary questions (e.g. where to start, what next)\\n\\nWhile you wait for answers from the community, check out the [FAQ](https://www.reddit.com/r/datascience/wiki/frequently-asked-questions) and [Resources](Resources) pages on our wiki. You can also search for answers in [past weekly threads](https://www.reddit.com/r/datascience/search?q=weekly%20thread&restrict_sr=1&sort=new).\n",
       "506                                       Is there any value in understanding and applying the underlying theory?  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    So to expand, I have a project idea of doing a fairly simple linear regression, but not in 3 lines of code.\\n\\nI'm interested to see if I can use my academic understanding to do this. so going from the data to minimising my SSR to then solving this system of equations using the OLS formula of (X'X)^{-1}X'Y1 to obtain my OLS estimate. \\n\\nIn industry it seems redundant but as a side project would this be valuable or make me seem a little more engaged.\\n\\nI am just trying to gauge whether it is s good use of my time.\n",
       "\n",
       "[507 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', None)\n",
    "df = []\n",
    "\n",
    "subreddit = reddit.subreddit('datascience')\n",
    "\n",
    "for post in subreddit.hot(limit=1000):\n",
    "    df.append([post.title, post.score, post.url, post.num_comments, post.selftext])\n",
    "\n",
    "df = pd.DataFrame(df,columns=['title', 'score', 'url', 'num_comments', 'body'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXxP8Uudb-cT"
   },
   "source": [
    "## Finding all the \"Question\" Topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2EzGOC1KD18"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnsW2kNcKsX0"
   },
   "outputs": [],
   "source": [
    "df1 = df[df['title'].str.contains('^.*[?]')==True]\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rH9d-pbPOjuc"
   },
   "outputs": [],
   "source": [
    "df1.sort_values(by=['score'], inplace=True, ascending=False)\n",
    "df1[['title', 'score','num_comments','body']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "community_list = None\n",
    "widgets = reddit.subreddit(\"test\").widgets\n",
    "for widget in widgets.sidebar:\n",
    "    if isinstance(widget, praw.models.CommunityList):\n",
    "        community_list = widget\n",
    "        break\n",
    "\n",
    "print(community_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  score      id subreddit  \\\n",
      "0  Frequently Asked Questions and Information Thread    509  js6jft       btc   \n",
      "\n",
      "                                                 url  num_comments  \\\n",
      "0  https://www.reddit.com/r/btc/comments/js6jft/f...          1843   \n",
      "\n",
      "                                                body       created  \n",
      "0  This FAQ and information thread serves to info...  1.605094e+09  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
      "C:\\Users\\seokju gist\\AppData\\Local\\Temp\\ipykernel_2732\\808848923.py:8: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n"
     ]
    }
   ],
   "source": [
    "c_list = ['bitcoin', 'btc']\n",
    "\n",
    "\n",
    "for i in c_list:\n",
    "    posts = []\n",
    "    ml_subreddit = reddit.subreddit(i)\n",
    "    for post in ml_subreddit.hot(limit=10):\n",
    "        posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "        posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "    posts.to_csv('bitcoin_test.csv', index=True, encoding='utf-8')\n",
    "print(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psaw\n",
      "  Downloading psaw-0.1.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\seokju gist\\anaconda3\\envs\\mitoenv\\lib\\site-packages (from psaw) (2.28.1)\n",
      "Requirement already satisfied: Click in c:\\users\\seokju gist\\anaconda3\\envs\\mitoenv\\lib\\site-packages (from psaw) (8.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\seokju gist\\anaconda3\\envs\\mitoenv\\lib\\site-packages (from Click->psaw) (0.4.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\seokju gist\\anaconda3\\envs\\mitoenv\\lib\\site-packages (from requests->psaw) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\seokju gist\\anaconda3\\envs\\mitoenv\\lib\\site-packages (from requests->psaw) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\seokju gist\\anaconda3\\envs\\mitoenv\\lib\\site-packages (from requests->psaw) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\seokju gist\\anaconda3\\envs\\mitoenv\\lib\\site-packages (from requests->psaw) (2.1.1)\n",
      "Installing collected packages: psaw\n",
      "Successfully installed psaw-0.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing requirements for soupsieve: [Errno 2] No such file or directory: 'c:\\\\users\\\\seokju gist\\\\anaconda3\\\\envs\\\\mitoenv\\\\lib\\\\site-packages\\\\soupsieve-2.3.2.post1.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing requirements for pyzmq: [Errno 2] No such file or directory: 'c:\\\\users\\\\seokju gist\\\\anaconda3\\\\envs\\\\mitoenv\\\\lib\\\\site-packages\\\\pyzmq-23.2.1.dist-info\\\\METADATA'\n"
     ]
    }
   ],
   "source": [
    "!pip3 install psaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Year]2020\n",
      "\t[Subeddit]bitcoin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seokju gist\\anaconda3\\envs\\mitoenv\\lib\\site-packages\\psaw\\PushshiftAPI.py:252: UserWarning: Not all PushShift shards are active. Query results may be incomplete\n",
      "  warnings.warn(shards_down_message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t[Info] Found submissions: 1\n",
      "\t\t[Info] Elapsed time:  2.68s\n",
      "\t\t[Info] Found submissions: 2\n",
      "\t\t[Info] Elapsed time:  2.99s\n",
      "\t\t[Info] Found submissions: 3\n",
      "\t\t[Info] Elapsed time:  3.41s\n",
      "\t\t[Info] Found submissions: 4\n",
      "\t\t[Info] Elapsed time:  3.94s\n",
      "\t\t[Info] Found submissions: 5\n",
      "\t\t[Info] Elapsed time:  4.31s\n",
      "\t\t[Info] Found submissions: 6\n",
      "\t\t[Info] Elapsed time:  4.57s\n",
      "\t\t[Info] Found submissions: 7\n",
      "\t\t[Info] Elapsed time:  4.85s\n",
      "\t\t[Info] Found submissions: 8\n",
      "\t\t[Info] Elapsed time:  5.15s\n",
      "\t\t[Info] Found submissions: 9\n",
      "\t\t[Info] Elapsed time:  5.42s\n",
      "\t\t[Info] Found submissions: 10\n",
      "\t\t[Info] Elapsed time:  5.89s\n",
      "\t\t[Info] Found submissions: 11\n",
      "\t\t[Info] Elapsed time:  6.21s\n",
      "\t\t[Info] Found submissions: 12\n",
      "\t\t[Info] Elapsed time:  6.47s\n",
      "\t\t[Info] Found submissions: 13\n",
      "\t\t[Info] Elapsed time:  6.81s\n",
      "\t\t[Info] Found submissions: 14\n",
      "\t\t[Info] Elapsed time:  7.24s\n",
      "\t\t[Info] Found submissions: 15\n",
      "\t\t[Info] Elapsed time:  7.58s\n",
      "\t\t[Info] Found submissions: 16\n",
      "\t\t[Info] Elapsed time:  8.07s\n",
      "\t\t[Info] Found submissions: 17\n",
      "\t\t[Info] Elapsed time:  8.41s\n",
      "\t\t[Info] Found submissions: 18\n",
      "\t\t[Info] Elapsed time:  8.72s\n",
      "\t\t[Info] Found submissions: 19\n",
      "\t\t[Info] Elapsed time:  9.11s\n",
      "\t\t[Info] Found submissions: 20\n",
      "\t\t[Info] Elapsed time:  9.43s\n",
      "\t\t[Info] Found submissions: 21\n",
      "\t\t[Info] Elapsed time:  9.81s\n",
      "\t\t[Info] Found submissions: 22\n",
      "\t\t[Info] Elapsed time:  10.07s\n",
      "\t\t[Info] Found submissions: 23\n",
      "\t\t[Info] Elapsed time:  10.38s\n",
      "\t\t[Info] Found submissions: 24\n",
      "\t\t[Info] Elapsed time:  10.65s\n",
      "\t\t[Info] Found submissions: 25\n",
      "\t\t[Info] Elapsed time:  11.90s\n",
      "\t\t[Info] Found submissions: 26\n",
      "\t\t[Info] Elapsed time:  12.30s\n",
      "\t\t[Info] Found submissions: 27\n",
      "\t\t[Info] Elapsed time:  12.70s\n",
      "\t\t[Info] Found submissions: 28\n",
      "\t\t[Info] Elapsed time:  12.95s\n",
      "\t\t[Info] Found submissions: 29\n",
      "\t\t[Info] Elapsed time:  13.21s\n",
      "\t\t[Info] Found submissions: 30\n",
      "\t\t[Info] Elapsed time:  13.48s\n",
      "\t\t[Info] Found submissions: 31\n",
      "\t\t[Info] Elapsed time:  13.78s\n",
      "\t\t[Info] Found submissions: 32\n",
      "\t\t[Info] Elapsed time:  14.05s\n",
      "\t\t[Info] Found submissions: 33\n",
      "\t\t[Info] Elapsed time:  14.30s\n",
      "\t\t[Info] Found submissions: 34\n",
      "\t\t[Info] Elapsed time:  14.63s\n",
      "\t\t[Info] Found submissions: 35\n",
      "\t\t[Info] Elapsed time:  14.98s\n",
      "\t\t[Info] Found submissions: 36\n",
      "\t\t[Info] Elapsed time:  15.46s\n",
      "\t\t[Info] Found submissions: 37\n",
      "\t\t[Info] Elapsed time:  15.94s\n",
      "\t\t[Info] Found submissions: 38\n",
      "\t\t[Info] Elapsed time:  16.29s\n",
      "\t\t[Info] Found submissions: 39\n",
      "\t\t[Info] Elapsed time:  16.55s\n",
      "\t\t[Info] Found submissions: 40\n",
      "\t\t[Info] Elapsed time:  16.82s\n",
      "\t\t[Info] Found submissions: 41\n",
      "\t\t[Info] Elapsed time:  17.11s\n",
      "\t\t[Info] Found submissions: 42\n",
      "\t\t[Info] Elapsed time:  17.43s\n",
      "\t\t[Info] Found submissions: 43\n",
      "\t\t[Info] Elapsed time:  17.71s\n",
      "\t\t[Info] Found submissions: 44\n",
      "\t\t[Info] Elapsed time:  17.98s\n",
      "\t\t[Info] Found submissions: 45\n",
      "\t\t[Info] Elapsed time:  18.44s\n",
      "\t\t[Info] Found submissions: 46\n",
      "\t\t[Info] Elapsed time:  18.86s\n",
      "\t\t[Info] Found submissions: 47\n",
      "\t\t[Info] Elapsed time:  19.28s\n",
      "\t\t[Info] Found submissions: 48\n",
      "\t\t[Info] Elapsed time:  19.55s\n",
      "\t\t[Info] Found submissions: 49\n",
      "\t\t[Info] Elapsed time:  19.89s\n",
      "\t\t[Info] Found submissions: 50\n",
      "\t\t[Info] Elapsed time:  20.18s\n",
      "\t\t[Info] Found submissions: 51\n",
      "\t\t[Info] Elapsed time:  20.46s\n",
      "\t\t[Info] Found submissions: 52\n",
      "\t\t[Info] Elapsed time:  20.98s\n",
      "\t\t[Info] Found submissions: 53\n",
      "\t\t[Info] Elapsed time:  21.28s\n",
      "\t\t[Info] Found submissions: 54\n",
      "\t\t[Info] Elapsed time:  21.82s\n",
      "\t\t[Info] Found submissions: 55\n",
      "\t\t[Info] Elapsed time:  22.12s\n",
      "\t\t[Info] Found submissions: 56\n",
      "\t\t[Info] Elapsed time:  22.45s\n",
      "\t\t[Info] Found submissions: 57\n",
      "\t\t[Info] Elapsed time:  22.78s\n",
      "\t\t[Info] Found submissions: 58\n",
      "\t\t[Info] Elapsed time:  23.11s\n",
      "\t\t[Info] Found submissions: 59\n",
      "\t\t[Info] Elapsed time:  23.40s\n",
      "\t\t[Info] Found submissions: 60\n",
      "\t\t[Info] Elapsed time:  23.73s\n",
      "\t\t[Info] Found submissions: 61\n",
      "\t\t[Info] Elapsed time:  23.98s\n",
      "\t\t[Info] Found submissions: 62\n",
      "\t\t[Info] Elapsed time:  24.41s\n",
      "\t\t[Info] Found submissions: 63\n",
      "\t\t[Info] Elapsed time:  24.78s\n",
      "\t\t[Info] Found submissions: 64\n",
      "\t\t[Info] Elapsed time:  25.41s\n",
      "\t\t[Info] Found submissions: 65\n",
      "\t\t[Info] Elapsed time:  25.82s\n",
      "\t\t[Info] Found submissions: 66\n",
      "\t\t[Info] Elapsed time:  26.08s\n",
      "\t\t[Info] Found submissions: 67\n",
      "\t\t[Info] Elapsed time:  26.41s\n",
      "\t\t[Info] Found submissions: 68\n",
      "\t\t[Info] Elapsed time:  26.68s\n",
      "\t\t[Info] Found submissions: 69\n",
      "\t\t[Info] Elapsed time:  26.95s\n",
      "\t\t[Info] Found submissions: 70\n",
      "\t\t[Info] Elapsed time:  27.42s\n",
      "\t\t[Info] Found submissions: 71\n",
      "\t\t[Info] Elapsed time:  27.69s\n",
      "\t\t[Info] Found submissions: 72\n",
      "\t\t[Info] Elapsed time:  28.00s\n",
      "\t\t[Info] Found submissions: 73\n",
      "\t\t[Info] Elapsed time:  28.32s\n",
      "\t\t[Info] Found submissions: 74\n",
      "\t\t[Info] Elapsed time:  28.60s\n",
      "\t\t[Info] Found submissions: 75\n",
      "\t\t[Info] Elapsed time:  28.91s\n",
      "\t\t[Info] Found submissions: 76\n",
      "\t\t[Info] Elapsed time:  29.43s\n",
      "\t\t[Info] Found submissions: 77\n",
      "\t\t[Info] Elapsed time:  29.69s\n",
      "\t\t[Info] Found submissions: 78\n",
      "\t\t[Info] Elapsed time:  30.03s\n",
      "\t\t[Info] Found submissions: 79\n",
      "\t\t[Info] Elapsed time:  30.46s\n",
      "\t\t[Info] Found submissions: 80\n",
      "\t\t[Info] Elapsed time:  30.82s\n",
      "\t\t[Info] Found submissions: 81\n",
      "\t\t[Info] Elapsed time:  31.08s\n",
      "\t\t[Info] Found submissions: 82\n",
      "\t\t[Info] Elapsed time:  31.44s\n",
      "\t\t[Info] Found submissions: 83\n",
      "\t\t[Info] Elapsed time:  31.86s\n",
      "\t\t[Info] Found submissions: 84\n",
      "\t\t[Info] Elapsed time:  32.29s\n",
      "\t\t[Info] Found submissions: 85\n",
      "\t\t[Info] Elapsed time:  32.55s\n",
      "\t\t[Info] Found submissions: 86\n",
      "\t\t[Info] Elapsed time:  33.03s\n",
      "\t\t[Info] Found submissions: 87\n",
      "\t\t[Info] Elapsed time:  33.88s\n",
      "\t\t[Info] Found submissions: 88\n",
      "\t\t[Info] Elapsed time:  34.17s\n",
      "\t\t[Info] Found submissions: 89\n",
      "\t\t[Info] Elapsed time:  34.52s\n",
      "\t\t[Info] Found submissions: 90\n",
      "\t\t[Info] Elapsed time:  34.92s\n",
      "\t\t[Info] Found submissions: 91\n",
      "\t\t[Info] Elapsed time:  35.19s\n",
      "\t\t[Info] Found submissions: 92\n",
      "\t\t[Info] Elapsed time:  35.45s\n",
      "\t\t[Info] Found submissions: 93\n",
      "\t\t[Info] Elapsed time:  35.79s\n",
      "\t\t[Info] Found submissions: 94\n",
      "\t\t[Info] Elapsed time:  36.04s\n",
      "\t\t[Info] Found submissions: 95\n",
      "\t\t[Info] Elapsed time:  36.32s\n",
      "\t\t[Info] Found submissions: 96\n",
      "\t\t[Info] Elapsed time:  36.63s\n",
      "\t\t[Info] Found submissions: 97\n",
      "\t\t[Info] Elapsed time:  37.03s\n",
      "\t\t[Info] Found submissions: 98\n",
      "\t\t[Info] Elapsed time:  37.76s\n",
      "\t\t[Info] Found submissions: 99\n",
      "\t\t[Info] Elapsed time:  38.02s\n",
      "\t\t[Info] Found submissions: 100\n",
      "\t\t[Info] Elapsed time:  38.32s\n",
      "\t[Subeddit]btc\n",
      "\t\t[Info] Found submissions: 1\n",
      "\t\t[Info] Elapsed time:  1.09s\n",
      "\t\t[Info] Found submissions: 2\n",
      "\t\t[Info] Elapsed time:  1.38s\n",
      "\t\t[Info] Found submissions: 3\n",
      "\t\t[Info] Elapsed time:  1.70s\n",
      "\t\t[Info] Found submissions: 4\n",
      "\t\t[Info] Elapsed time:  2.16s\n",
      "\t\t[Info] Found submissions: 5\n",
      "\t\t[Info] Elapsed time:  2.64s\n",
      "\t\t[Info] Found submissions: 6\n",
      "\t\t[Info] Elapsed time:  2.90s\n",
      "\t\t[Info] Found submissions: 7\n",
      "\t\t[Info] Elapsed time:  3.20s\n",
      "\t\t[Info] Found submissions: 8\n",
      "\t\t[Info] Elapsed time:  3.46s\n",
      "\t\t[Info] Found submissions: 9\n",
      "\t\t[Info] Elapsed time:  3.72s\n",
      "\t\t[Info] Found submissions: 10\n",
      "\t\t[Info] Elapsed time:  4.08s\n",
      "\t\t[Info] Found submissions: 11\n",
      "\t\t[Info] Elapsed time:  4.42s\n",
      "\t\t[Info] Found submissions: 12\n",
      "\t\t[Info] Elapsed time:  4.68s\n",
      "\t\t[Info] Found submissions: 13\n",
      "\t\t[Info] Elapsed time:  5.01s\n",
      "\t\t[Info] Found submissions: 14\n",
      "\t\t[Info] Elapsed time:  5.27s\n",
      "\t\t[Info] Found submissions: 15\n",
      "\t\t[Info] Elapsed time:  5.61s\n",
      "\t\t[Info] Found submissions: 16\n",
      "\t\t[Info] Elapsed time:  5.89s\n",
      "\t\t[Info] Found submissions: 17\n",
      "\t\t[Info] Elapsed time:  6.21s\n",
      "\t\t[Info] Found submissions: 18\n",
      "\t\t[Info] Elapsed time:  6.55s\n",
      "\t\t[Info] Found submissions: 19\n",
      "\t\t[Info] Elapsed time:  6.81s\n",
      "\t\t[Info] Found submissions: 20\n",
      "\t\t[Info] Elapsed time:  7.77s\n",
      "\t\t[Info] Found submissions: 21\n",
      "\t\t[Info] Elapsed time:  9.06s\n",
      "\t\t[Info] Found submissions: 22\n",
      "\t\t[Info] Elapsed time:  10.10s\n",
      "\t\t[Info] Found submissions: 23\n",
      "\t\t[Info] Elapsed time:  10.41s\n",
      "\t\t[Info] Found submissions: 24\n",
      "\t\t[Info] Elapsed time:  10.68s\n",
      "\t\t[Info] Found submissions: 25\n",
      "\t\t[Info] Elapsed time:  10.95s\n",
      "\t\t[Info] Found submissions: 26\n",
      "\t\t[Info] Elapsed time:  11.22s\n",
      "\t\t[Info] Found submissions: 27\n",
      "\t\t[Info] Elapsed time:  11.53s\n",
      "\t\t[Info] Found submissions: 28\n",
      "\t\t[Info] Elapsed time:  11.89s\n",
      "\t\t[Info] Found submissions: 29\n",
      "\t\t[Info] Elapsed time:  12.30s\n",
      "\t\t[Info] Found submissions: 30\n",
      "\t\t[Info] Elapsed time:  12.68s\n",
      "\t\t[Info] Found submissions: 31\n",
      "\t\t[Info] Elapsed time:  13.04s\n",
      "\t\t[Info] Found submissions: 32\n",
      "\t\t[Info] Elapsed time:  13.36s\n",
      "\t\t[Info] Found submissions: 33\n",
      "\t\t[Info] Elapsed time:  13.63s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t[Info] Found submissions: 34\n",
      "\t\t[Info] Elapsed time:  13.89s\n",
      "\t\t[Info] Found submissions: 35\n",
      "\t\t[Info] Elapsed time:  14.15s\n",
      "\t\t[Info] Found submissions: 36\n",
      "\t\t[Info] Elapsed time:  14.44s\n",
      "\t\t[Info] Found submissions: 37\n",
      "\t\t[Info] Elapsed time:  14.78s\n",
      "\t\t[Info] Found submissions: 38\n",
      "\t\t[Info] Elapsed time:  15.10s\n",
      "\t\t[Info] Found submissions: 39\n",
      "\t\t[Info] Elapsed time:  15.76s\n",
      "\t\t[Info] Found submissions: 40\n",
      "\t\t[Info] Elapsed time:  16.10s\n",
      "\t\t[Info] Found submissions: 41\n",
      "\t\t[Info] Elapsed time:  16.36s\n",
      "\t\t[Info] Found submissions: 42\n",
      "\t\t[Info] Elapsed time:  16.89s\n",
      "\t\t[Info] Found submissions: 43\n",
      "\t\t[Info] Elapsed time:  17.26s\n",
      "\t\t[Info] Found submissions: 44\n",
      "\t\t[Info] Elapsed time:  17.67s\n",
      "\t\t[Info] Found submissions: 45\n",
      "\t\t[Info] Elapsed time:  18.06s\n",
      "\t\t[Info] Found submissions: 46\n",
      "\t\t[Info] Elapsed time:  18.41s\n",
      "\t\t[Info] Found submissions: 47\n",
      "\t\t[Info] Elapsed time:  18.75s\n",
      "\t\t[Info] Found submissions: 48\n",
      "\t\t[Info] Elapsed time:  19.02s\n",
      "\t\t[Info] Found submissions: 49\n",
      "\t\t[Info] Elapsed time:  19.29s\n",
      "\t\t[Info] Found submissions: 50\n",
      "\t\t[Info] Elapsed time:  19.74s\n",
      "\t\t[Info] Found submissions: 51\n",
      "\t\t[Info] Elapsed time:  20.65s\n",
      "\t\t[Info] Found submissions: 52\n",
      "\t\t[Info] Elapsed time:  20.97s\n",
      "\t\t[Info] Found submissions: 53\n",
      "\t\t[Info] Elapsed time:  21.34s\n",
      "\t\t[Info] Found submissions: 54\n",
      "\t\t[Info] Elapsed time:  21.60s\n",
      "\t\t[Info] Found submissions: 55\n",
      "\t\t[Info] Elapsed time:  21.97s\n",
      "\t\t[Info] Found submissions: 56\n",
      "\t\t[Info] Elapsed time:  22.51s\n",
      "\t\t[Info] Found submissions: 57\n",
      "\t\t[Info] Elapsed time:  22.76s\n",
      "\t\t[Info] Found submissions: 58\n",
      "\t\t[Info] Elapsed time:  23.31s\n",
      "\t\t[Info] Found submissions: 59\n",
      "\t\t[Info] Elapsed time:  23.59s\n",
      "\t\t[Info] Found submissions: 60\n",
      "\t\t[Info] Elapsed time:  23.90s\n",
      "\t\t[Info] Found submissions: 61\n",
      "\t\t[Info] Elapsed time:  24.20s\n",
      "\t\t[Info] Found submissions: 62\n",
      "\t\t[Info] Elapsed time:  24.46s\n",
      "\t\t[Info] Found submissions: 63\n",
      "\t\t[Info] Elapsed time:  24.73s\n",
      "\t\t[Info] Found submissions: 64\n",
      "\t\t[Info] Elapsed time:  25.18s\n",
      "\t\t[Info] Found submissions: 65\n",
      "\t\t[Info] Elapsed time:  25.57s\n",
      "\t\t[Info] Found submissions: 66\n",
      "\t\t[Info] Elapsed time:  25.96s\n",
      "\t\t[Info] Found submissions: 67\n",
      "\t\t[Info] Elapsed time:  26.24s\n",
      "\t\t[Info] Found submissions: 68\n",
      "\t\t[Info] Elapsed time:  26.90s\n",
      "\t\t[Info] Found submissions: 69\n",
      "\t\t[Info] Elapsed time:  27.16s\n",
      "\t\t[Info] Found submissions: 70\n",
      "\t\t[Info] Elapsed time:  27.42s\n",
      "\t\t[Info] Found submissions: 71\n",
      "\t\t[Info] Elapsed time:  27.68s\n",
      "\t\t[Info] Found submissions: 72\n",
      "\t\t[Info] Elapsed time:  28.04s\n",
      "\t\t[Info] Found submissions: 73\n",
      "\t\t[Info] Elapsed time:  28.43s\n",
      "\t\t[Info] Found submissions: 74\n",
      "\t\t[Info] Elapsed time:  28.86s\n",
      "\t\t[Info] Found submissions: 75\n",
      "\t\t[Info] Elapsed time:  29.19s\n",
      "\t\t[Info] Found submissions: 76\n",
      "\t\t[Info] Elapsed time:  29.49s\n",
      "\t\t[Info] Found submissions: 77\n",
      "\t\t[Info] Elapsed time:  30.26s\n",
      "\t\t[Info] Found submissions: 78\n",
      "\t\t[Info] Elapsed time:  30.53s\n",
      "\t\t[Info] Found submissions: 79\n",
      "\t\t[Info] Elapsed time:  30.93s\n",
      "\t\t[Info] Found submissions: 80\n",
      "\t\t[Info] Elapsed time:  31.19s\n",
      "\t\t[Info] Found submissions: 81\n",
      "\t\t[Info] Elapsed time:  31.46s\n",
      "\t\t[Info] Found submissions: 82\n",
      "\t\t[Info] Elapsed time:  31.81s\n",
      "\t\t[Info] Found submissions: 83\n",
      "\t\t[Info] Elapsed time:  32.06s\n",
      "\t\t[Info] Found submissions: 84\n",
      "\t\t[Info] Elapsed time:  32.31s\n",
      "\t\t[Info] Found submissions: 85\n",
      "\t\t[Info] Elapsed time:  32.58s\n",
      "\t\t[Info] Found submissions: 86\n",
      "\t\t[Info] Elapsed time:  32.84s\n",
      "\t\t[Info] Found submissions: 87\n",
      "\t\t[Info] Elapsed time:  33.23s\n",
      "\t\t[Info] Found submissions: 88\n",
      "\t\t[Info] Elapsed time:  33.64s\n",
      "\t\t[Info] Found submissions: 89\n",
      "\t\t[Info] Elapsed time:  33.95s\n",
      "\t\t[Info] Found submissions: 90\n",
      "\t\t[Info] Elapsed time:  34.21s\n",
      "\t\t[Info] Found submissions: 91\n",
      "\t\t[Info] Elapsed time:  34.60s\n",
      "\t\t[Info] Found submissions: 92\n",
      "\t\t[Info] Elapsed time:  34.95s\n",
      "\t\t[Info] Found submissions: 93\n",
      "\t\t[Info] Elapsed time:  35.37s\n",
      "\t\t[Info] Found submissions: 94\n",
      "\t\t[Info] Elapsed time:  35.63s\n",
      "\t\t[Info] Found submissions: 95\n",
      "\t\t[Info] Elapsed time:  36.02s\n",
      "\t\t[Info] Found submissions: 96\n",
      "\t\t[Info] Elapsed time:  36.42s\n",
      "\t\t[Info] Found submissions: 97\n",
      "\t\t[Info] Elapsed time:  36.76s\n",
      "\t\t[Info] Found submissions: 98\n",
      "\t\t[Info] Elapsed time:  37.98s\n",
      "\t\t[Info] Found submissions: 99\n",
      "\t\t[Info] Elapsed time:  38.65s\n",
      "\t\t[Info] Found submissions: 100\n",
      "\t\t[Info] Elapsed time:  38.96s\n",
      "[Year]2021\n",
      "\t[Subeddit]bitcoin\n",
      "\t\t[Info] Found submissions: 1\n",
      "\t\t[Info] Elapsed time:  1.38s\n",
      "\t\t[Info] Found submissions: 2\n",
      "\t\t[Info] Elapsed time:  1.65s\n",
      "\t\t[Info] Found submissions: 3\n",
      "\t\t[Info] Elapsed time:  1.97s\n",
      "\t\t[Info] Found submissions: 4\n",
      "\t\t[Info] Elapsed time:  2.23s\n",
      "\t\t[Info] Found submissions: 5\n",
      "\t\t[Info] Elapsed time:  2.52s\n",
      "\t\t[Info] Found submissions: 6\n",
      "\t\t[Info] Elapsed time:  2.76s\n",
      "\t\t[Info] Found submissions: 7\n",
      "\t\t[Info] Elapsed time:  3.65s\n",
      "\t\t[Info] Found submissions: 8\n",
      "\t\t[Info] Elapsed time:  4.50s\n",
      "\t\t[Info] Found submissions: 9\n",
      "\t\t[Info] Elapsed time:  5.28s\n",
      "\t\t[Info] Found submissions: 10\n",
      "\t\t[Info] Elapsed time:  6.15s\n",
      "\t\t[Info] Found submissions: 11\n",
      "\t\t[Info] Elapsed time:  6.95s\n",
      "\t\t[Info] Found submissions: 12\n",
      "\t\t[Info] Elapsed time:  7.73s\n",
      "\t\t[Info] Found submissions: 13\n",
      "\t\t[Info] Elapsed time:  8.98s\n",
      "\t\t[Info] Found submissions: 14\n",
      "\t\t[Info] Elapsed time:  10.28s\n",
      "\t\t[Info] Found submissions: 15\n",
      "\t\t[Info] Elapsed time:  11.58s\n",
      "\t\t[Info] Found submissions: 16\n",
      "\t\t[Info] Elapsed time:  12.34s\n",
      "\t\t[Info] Found submissions: 17\n",
      "\t\t[Info] Elapsed time:  13.21s\n",
      "\t\t[Info] Found submissions: 18\n",
      "\t\t[Info] Elapsed time:  15.04s\n",
      "\t\t[Info] Found submissions: 19\n",
      "\t\t[Info] Elapsed time:  15.84s\n",
      "\t\t[Info] Found submissions: 20\n",
      "\t\t[Info] Elapsed time:  17.16s\n",
      "\t\t[Info] Found submissions: 21\n",
      "\t\t[Info] Elapsed time:  18.43s\n",
      "\t\t[Info] Found submissions: 22\n",
      "\t\t[Info] Elapsed time:  19.31s\n",
      "\t\t[Info] Found submissions: 23\n",
      "\t\t[Info] Elapsed time:  20.12s\n",
      "\t\t[Info] Found submissions: 24\n",
      "\t\t[Info] Elapsed time:  20.94s\n",
      "\t\t[Info] Found submissions: 25\n",
      "\t\t[Info] Elapsed time:  21.70s\n",
      "\t\t[Info] Found submissions: 26\n",
      "\t\t[Info] Elapsed time:  23.02s\n",
      "\t\t[Info] Found submissions: 27\n",
      "\t\t[Info] Elapsed time:  23.83s\n",
      "\t\t[Info] Found submissions: 28\n",
      "\t\t[Info] Elapsed time:  25.12s\n",
      "\t\t[Info] Found submissions: 29\n",
      "\t\t[Info] Elapsed time:  25.93s\n",
      "\t\t[Info] Found submissions: 30\n",
      "\t\t[Info] Elapsed time:  26.72s\n",
      "\t\t[Info] Found submissions: 31\n",
      "\t\t[Info] Elapsed time:  27.98s\n",
      "\t\t[Info] Found submissions: 32\n",
      "\t\t[Info] Elapsed time:  28.80s\n",
      "\t\t[Info] Found submissions: 33\n",
      "\t\t[Info] Elapsed time:  30.10s\n",
      "\t\t[Info] Found submissions: 34\n",
      "\t\t[Info] Elapsed time:  30.90s\n",
      "\t\t[Info] Found submissions: 35\n",
      "\t\t[Info] Elapsed time:  31.71s\n",
      "\t\t[Info] Found submissions: 36\n",
      "\t\t[Info] Elapsed time:  33.20s\n",
      "\t\t[Info] Found submissions: 37\n",
      "\t\t[Info] Elapsed time:  33.97s\n",
      "\t\t[Info] Found submissions: 38\n",
      "\t\t[Info] Elapsed time:  35.27s\n",
      "\t\t[Info] Found submissions: 39\n",
      "\t\t[Info] Elapsed time:  36.56s\n",
      "\t\t[Info] Found submissions: 40\n",
      "\t\t[Info] Elapsed time:  37.33s\n",
      "\t\t[Info] Found submissions: 41\n",
      "\t\t[Info] Elapsed time:  38.19s\n",
      "\t\t[Info] Found submissions: 42\n",
      "\t\t[Info] Elapsed time:  39.04s\n",
      "\t\t[Info] Found submissions: 43\n",
      "\t\t[Info] Elapsed time:  39.85s\n",
      "\t\t[Info] Found submissions: 44\n",
      "\t\t[Info] Elapsed time:  41.24s\n",
      "\t\t[Info] Found submissions: 45\n",
      "\t\t[Info] Elapsed time:  42.05s\n",
      "\t\t[Info] Found submissions: 46\n",
      "\t\t[Info] Elapsed time:  42.82s\n",
      "\t\t[Info] Found submissions: 47\n",
      "\t\t[Info] Elapsed time:  44.21s\n",
      "\t\t[Info] Found submissions: 48\n",
      "\t\t[Info] Elapsed time:  44.98s\n",
      "\t\t[Info] Found submissions: 49\n",
      "\t\t[Info] Elapsed time:  45.86s\n",
      "\t\t[Info] Found submissions: 50\n",
      "\t\t[Info] Elapsed time:  47.16s\n",
      "\t\t[Info] Found submissions: 51\n",
      "\t\t[Info] Elapsed time:  47.92s\n",
      "\t\t[Info] Found submissions: 52\n",
      "\t\t[Info] Elapsed time:  48.69s\n",
      "\t\t[Info] Found submissions: 53\n",
      "\t\t[Info] Elapsed time:  49.95s\n",
      "\t\t[Info] Found submissions: 54\n",
      "\t\t[Info] Elapsed time:  50.89s\n",
      "\t\t[Info] Found submissions: 55\n",
      "\t\t[Info] Elapsed time:  52.14s\n",
      "\t\t[Info] Found submissions: 56\n",
      "\t\t[Info] Elapsed time:  52.97s\n",
      "\t\t[Info] Found submissions: 57\n",
      "\t\t[Info] Elapsed time:  54.12s\n",
      "\t\t[Info] Found submissions: 58\n",
      "\t\t[Info] Elapsed time:  55.49s\n",
      "\t\t[Info] Found submissions: 59\n",
      "\t\t[Info] Elapsed time:  56.26s\n",
      "\t\t[Info] Found submissions: 60\n",
      "\t\t[Info] Elapsed time:  57.17s\n",
      "\t\t[Info] Found submissions: 61\n",
      "\t\t[Info] Elapsed time:  58.03s\n",
      "\t\t[Info] Found submissions: 62\n",
      "\t\t[Info] Elapsed time:  58.81s\n",
      "\t\t[Info] Found submissions: 63\n",
      "\t\t[Info] Elapsed time:  60.09s\n",
      "\t\t[Info] Found submissions: 64\n",
      "\t\t[Info] Elapsed time:  60.91s\n",
      "\t\t[Info] Found submissions: 65\n",
      "\t\t[Info] Elapsed time:  61.79s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t[Info] Found submissions: 66\n",
      "\t\t[Info] Elapsed time:  63.17s\n",
      "\t\t[Info] Found submissions: 67\n",
      "\t\t[Info] Elapsed time:  63.96s\n",
      "\t\t[Info] Found submissions: 68\n",
      "\t\t[Info] Elapsed time:  64.74s\n",
      "\t\t[Info] Found submissions: 69\n",
      "\t\t[Info] Elapsed time:  66.00s\n",
      "\t\t[Info] Found submissions: 70\n",
      "\t\t[Info] Elapsed time:  66.76s\n",
      "\t\t[Info] Found submissions: 71\n",
      "\t\t[Info] Elapsed time:  68.03s\n",
      "\t\t[Info] Found submissions: 72\n",
      "\t\t[Info] Elapsed time:  68.80s\n",
      "\t\t[Info] Found submissions: 73\n",
      "\t\t[Info] Elapsed time:  70.09s\n",
      "\t\t[Info] Found submissions: 74\n",
      "\t\t[Info] Elapsed time:  70.88s\n",
      "\t\t[Info] Found submissions: 75\n",
      "\t\t[Info] Elapsed time:  72.21s\n",
      "\t\t[Info] Found submissions: 76\n",
      "\t\t[Info] Elapsed time:  72.96s\n",
      "\t\t[Info] Found submissions: 77\n",
      "\t\t[Info] Elapsed time:  73.71s\n",
      "\t\t[Info] Found submissions: 78\n",
      "\t\t[Info] Elapsed time:  75.02s\n",
      "\t\t[Info] Found submissions: 79\n",
      "\t\t[Info] Elapsed time:  75.79s\n",
      "\t\t[Info] Found submissions: 80\n",
      "\t\t[Info] Elapsed time:  77.08s\n",
      "\t\t[Info] Found submissions: 81\n",
      "\t\t[Info] Elapsed time:  77.84s\n",
      "\t\t[Info] Found submissions: 82\n",
      "\t\t[Info] Elapsed time:  79.19s\n",
      "\t\t[Info] Found submissions: 83\n",
      "\t\t[Info] Elapsed time:  80.10s\n",
      "\t\t[Info] Found submissions: 84\n",
      "\t\t[Info] Elapsed time:  80.86s\n",
      "\t\t[Info] Found submissions: 85\n",
      "\t\t[Info] Elapsed time:  82.14s\n",
      "\t\t[Info] Found submissions: 86\n",
      "\t\t[Info] Elapsed time:  82.92s\n",
      "\t\t[Info] Found submissions: 87\n",
      "\t\t[Info] Elapsed time:  83.72s\n",
      "\t\t[Info] Found submissions: 88\n",
      "\t\t[Info] Elapsed time:  85.10s\n",
      "\t\t[Info] Found submissions: 89\n",
      "\t\t[Info] Elapsed time:  85.88s\n",
      "\t\t[Info] Found submissions: 90\n",
      "\t\t[Info] Elapsed time:  86.68s\n",
      "\t\t[Info] Found submissions: 91\n",
      "\t\t[Info] Elapsed time:  87.95s\n",
      "\t\t[Info] Found submissions: 92\n",
      "\t\t[Info] Elapsed time:  88.78s\n",
      "\t\t[Info] Found submissions: 93\n",
      "\t\t[Info] Elapsed time:  90.05s\n",
      "\t\t[Info] Found submissions: 94\n",
      "\t\t[Info] Elapsed time:  90.89s\n",
      "\t\t[Info] Found submissions: 95\n",
      "\t\t[Info] Elapsed time:  92.37s\n",
      "\t\t[Info] Found submissions: 96\n",
      "\t\t[Info] Elapsed time:  93.99s\n",
      "\t\t[Info] Found submissions: 97\n",
      "\t\t[Info] Elapsed time:  94.78s\n",
      "\t\t[Info] Found submissions: 98\n",
      "\t\t[Info] Elapsed time:  96.06s\n",
      "\t\t[Info] Found submissions: 99\n",
      "\t\t[Info] Elapsed time:  96.84s\n",
      "\t\t[Info] Found submissions: 100\n",
      "\t\t[Info] Elapsed time:  98.10s\n",
      "\t[Subeddit]btc\n",
      "\t\t[Info] Found submissions: 1\n",
      "\t\t[Info] Elapsed time:  1.32s\n",
      "\t\t[Info] Found submissions: 2\n",
      "\t\t[Info] Elapsed time:  2.40s\n",
      "\t\t[Info] Found submissions: 3\n",
      "\t\t[Info] Elapsed time:  3.17s\n",
      "\t\t[Info] Found submissions: 4\n",
      "\t\t[Info] Elapsed time:  4.02s\n",
      "\t\t[Info] Found submissions: 5\n",
      "\t\t[Info] Elapsed time:  4.80s\n",
      "\t\t[Info] Found submissions: 6\n",
      "\t\t[Info] Elapsed time:  5.60s\n",
      "\t\t[Info] Found submissions: 7\n",
      "\t\t[Info] Elapsed time:  6.87s\n",
      "\t\t[Info] Found submissions: 8\n",
      "\t\t[Info] Elapsed time:  7.66s\n",
      "\t\t[Info] Found submissions: 9\n",
      "\t\t[Info] Elapsed time:  8.96s\n",
      "\t\t[Info] Found submissions: 10\n",
      "\t\t[Info] Elapsed time:  9.82s\n",
      "\t\t[Info] Found submissions: 11\n",
      "\t\t[Info] Elapsed time:  11.09s\n",
      "\t\t[Info] Found submissions: 12\n",
      "\t\t[Info] Elapsed time:  11.87s\n",
      "\t\t[Info] Found submissions: 13\n",
      "\t\t[Info] Elapsed time:  12.65s\n",
      "\t\t[Info] Found submissions: 14\n",
      "\t\t[Info] Elapsed time:  14.04s\n",
      "\t\t[Info] Found submissions: 15\n",
      "\t\t[Info] Elapsed time:  14.83s\n",
      "\t\t[Info] Found submissions: 16\n",
      "\t\t[Info] Elapsed time:  15.60s\n",
      "\t\t[Info] Found submissions: 17\n",
      "\t\t[Info] Elapsed time:  16.89s\n",
      "\t\t[Info] Found submissions: 18\n",
      "\t\t[Info] Elapsed time:  17.65s\n",
      "\t\t[Info] Found submissions: 19\n",
      "\t\t[Info] Elapsed time:  19.03s\n",
      "\t\t[Info] Found submissions: 20\n",
      "\t\t[Info] Elapsed time:  19.80s\n",
      "\t\t[Info] Found submissions: 21\n",
      "\t\t[Info] Elapsed time:  20.60s\n",
      "\t\t[Info] Found submissions: 22\n",
      "\t\t[Info] Elapsed time:  23.00s\n",
      "\t\t[Info] Found submissions: 23\n",
      "\t\t[Info] Elapsed time:  23.89s\n",
      "\t\t[Info] Found submissions: 24\n",
      "\t\t[Info] Elapsed time:  24.65s\n",
      "\t\t[Info] Found submissions: 25\n",
      "\t\t[Info] Elapsed time:  25.93s\n",
      "\t\t[Info] Found submissions: 26\n",
      "\t\t[Info] Elapsed time:  26.70s\n",
      "\t\t[Info] Found submissions: 27\n",
      "\t\t[Info] Elapsed time:  28.00s\n",
      "\t\t[Info] Found submissions: 28\n",
      "\t\t[Info] Elapsed time:  28.79s\n",
      "\t\t[Info] Found submissions: 29\n",
      "\t\t[Info] Elapsed time:  29.81s\n",
      "\t\t[Info] Found submissions: 30\n",
      "\t\t[Info] Elapsed time:  31.60s\n",
      "\t\t[Info] Found submissions: 31\n",
      "\t\t[Info] Elapsed time:  32.34s\n",
      "\t\t[Info] Found submissions: 32\n",
      "\t\t[Info] Elapsed time:  33.94s\n",
      "\t\t[Info] Found submissions: 33\n",
      "\t\t[Info] Elapsed time:  34.69s\n",
      "\t\t[Info] Found submissions: 34\n",
      "\t\t[Info] Elapsed time:  35.45s\n",
      "\t\t[Info] Found submissions: 35\n",
      "\t\t[Info] Elapsed time:  36.22s\n",
      "\t\t[Info] Found submissions: 36\n",
      "\t\t[Info] Elapsed time:  36.98s\n",
      "\t\t[Info] Found submissions: 37\n",
      "\t\t[Info] Elapsed time:  37.74s\n",
      "\t\t[Info] Found submissions: 38\n",
      "\t\t[Info] Elapsed time:  39.06s\n",
      "\t\t[Info] Found submissions: 39\n",
      "\t\t[Info] Elapsed time:  40.06s\n",
      "\t\t[Info] Found submissions: 40\n",
      "\t\t[Info] Elapsed time:  40.85s\n",
      "\t\t[Info] Found submissions: 41\n",
      "\t\t[Info] Elapsed time:  41.63s\n",
      "\t\t[Info] Found submissions: 42\n",
      "\t\t[Info] Elapsed time:  42.90s\n",
      "\t\t[Info] Found submissions: 43\n",
      "\t\t[Info] Elapsed time:  43.82s\n",
      "\t\t[Info] Found submissions: 44\n",
      "\t\t[Info] Elapsed time:  45.23s\n",
      "\t\t[Info] Found submissions: 45\n",
      "\t\t[Info] Elapsed time:  46.01s\n",
      "\t\t[Info] Found submissions: 46\n",
      "\t\t[Info] Elapsed time:  46.77s\n",
      "\t\t[Info] Found submissions: 47\n",
      "\t\t[Info] Elapsed time:  47.64s\n",
      "\t\t[Info] Found submissions: 48\n",
      "\t\t[Info] Elapsed time:  48.90s\n",
      "\t\t[Info] Found submissions: 49\n",
      "\t\t[Info] Elapsed time:  50.15s\n",
      "\t\t[Info] Found submissions: 50\n",
      "\t\t[Info] Elapsed time:  51.46s\n",
      "\t\t[Info] Found submissions: 51\n",
      "\t\t[Info] Elapsed time:  52.26s\n",
      "\t\t[Info] Found submissions: 52\n",
      "\t\t[Info] Elapsed time:  53.10s\n",
      "\t\t[Info] Found submissions: 53\n",
      "\t\t[Info] Elapsed time:  54.16s\n",
      "\t\t[Info] Found submissions: 54\n",
      "\t\t[Info] Elapsed time:  54.92s\n",
      "\t\t[Info] Found submissions: 55\n",
      "\t\t[Info] Elapsed time:  55.80s\n",
      "\t\t[Info] Found submissions: 56\n",
      "\t\t[Info] Elapsed time:  57.07s\n",
      "\t\t[Info] Found submissions: 57\n",
      "\t\t[Info] Elapsed time:  57.91s\n",
      "\t\t[Info] Found submissions: 58\n",
      "\t\t[Info] Elapsed time:  58.67s\n",
      "\t\t[Info] Found submissions: 59\n",
      "\t\t[Info] Elapsed time:  59.98s\n",
      "\t\t[Info] Found submissions: 60\n",
      "\t\t[Info] Elapsed time:  60.74s\n",
      "\t\t[Info] Found submissions: 61\n",
      "\t\t[Info] Elapsed time:  62.04s\n",
      "\t\t[Info] Found submissions: 62\n",
      "\t\t[Info] Elapsed time:  62.79s\n",
      "\t\t[Info] Found submissions: 63\n",
      "\t\t[Info] Elapsed time:  63.56s\n",
      "\t\t[Info] Found submissions: 64\n",
      "\t\t[Info] Elapsed time:  64.97s\n",
      "\t\t[Info] Found submissions: 65\n",
      "\t\t[Info] Elapsed time:  65.90s\n",
      "\t\t[Info] Found submissions: 66\n",
      "\t\t[Info] Elapsed time:  66.72s\n",
      "\t\t[Info] Found submissions: 67\n",
      "\t\t[Info] Elapsed time:  68.08s\n",
      "\t\t[Info] Found submissions: 68\n",
      "\t\t[Info] Elapsed time:  68.87s\n",
      "\t\t[Info] Found submissions: 69\n",
      "\t\t[Info] Elapsed time:  69.68s\n",
      "\t\t[Info] Found submissions: 70\n",
      "\t\t[Info] Elapsed time:  71.00s\n",
      "\t\t[Info] Found submissions: 71\n",
      "\t\t[Info] Elapsed time:  72.46s\n",
      "\t\t[Info] Found submissions: 72\n",
      "\t\t[Info] Elapsed time:  73.24s\n",
      "\t\t[Info] Found submissions: 73\n",
      "\t\t[Info] Elapsed time:  74.10s\n",
      "\t\t[Info] Found submissions: 74\n",
      "\t\t[Info] Elapsed time:  74.86s\n",
      "\t\t[Info] Found submissions: 75\n",
      "\t\t[Info] Elapsed time:  75.90s\n",
      "\t\t[Info] Found submissions: 76\n",
      "\t\t[Info] Elapsed time:  77.18s\n",
      "\t\t[Info] Found submissions: 77\n",
      "\t\t[Info] Elapsed time:  77.97s\n",
      "\t\t[Info] Found submissions: 78\n",
      "\t\t[Info] Elapsed time:  78.75s\n",
      "\t\t[Info] Found submissions: 79\n",
      "\t\t[Info] Elapsed time:  80.04s\n",
      "\t\t[Info] Found submissions: 80\n",
      "\t\t[Info] Elapsed time:  80.82s\n",
      "\t\t[Info] Found submissions: 81\n",
      "\t\t[Info] Elapsed time:  81.71s\n",
      "\t\t[Info] Found submissions: 82\n",
      "\t\t[Info] Elapsed time:  82.99s\n",
      "\t\t[Info] Found submissions: 83\n",
      "\t\t[Info] Elapsed time:  83.77s\n",
      "\t\t[Info] Found submissions: 84\n",
      "\t\t[Info] Elapsed time:  85.12s\n",
      "\t\t[Info] Found submissions: 85\n",
      "\t\t[Info] Elapsed time:  85.92s\n",
      "\t\t[Info] Found submissions: 86\n",
      "\t\t[Info] Elapsed time:  86.78s\n",
      "\t\t[Info] Found submissions: 87\n",
      "\t\t[Info] Elapsed time:  88.15s\n",
      "\t\t[Info] Found submissions: 88\n",
      "\t\t[Info] Elapsed time:  88.98s\n",
      "\t\t[Info] Found submissions: 89\n",
      "\t\t[Info] Elapsed time:  89.75s\n",
      "\t\t[Info] Found submissions: 90\n",
      "\t\t[Info] Elapsed time:  91.17s\n",
      "\t\t[Info] Found submissions: 91\n",
      "\t\t[Info] Elapsed time:  91.94s\n",
      "\t\t[Info] Found submissions: 92\n",
      "\t\t[Info] Elapsed time:  92.72s\n",
      "\t\t[Info] Found submissions: 93\n",
      "\t\t[Info] Elapsed time:  93.99s\n",
      "\t\t[Info] Found submissions: 94\n",
      "\t\t[Info] Elapsed time:  94.77s\n",
      "\t\t[Info] Found submissions: 95\n",
      "\t\t[Info] Elapsed time:  95.61s\n",
      "\t\t[Info] Found submissions: 96\n",
      "\t\t[Info] Elapsed time:  96.87s\n",
      "\t\t[Info] Found submissions: 97\n",
      "\t\t[Info] Elapsed time:  97.80s\n",
      "\t\t[Info] Found submissions: 98\n",
      "\t\t[Info] Elapsed time:  99.09s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t[Info] Found submissions: 99\n",
      "\t\t[Info] Elapsed time:  99.87s\n",
      "\t\t[Info] Found submissions: 100\n",
      "\t\t[Info] Elapsed time:  100.64s\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "from psaw import PushshiftAPI\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# to use PSAW\n",
    "api = PushshiftAPI()\n",
    "\n",
    "# to use PRAW\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"9MKSo0tYTRaUWPrcNq8tpA\",\n",
    "    client_secret=\"5twxWoEj6Y4fvik1SbqA7Sac105PNg\",\n",
    "    password=\"nitsawahied374\",\n",
    "    user_agent=\"price App\",\n",
    "    username=\"wahid028\",\n",
    ")\n",
    "\n",
    "def log_action(action):\n",
    "    print(action)\n",
    "    return\n",
    "\n",
    "subreddits = ['bitcoin', 'btc']\n",
    "start_year = 2020\n",
    "end_year = 2021\n",
    "\n",
    "# directory on which to store the data\n",
    "basecorpus = './my-dataset/'\n",
    "\n",
    "for year in range(start_year, end_year+1):\n",
    "    action = '[Year]' + str(year)\n",
    "    log_action(action)\n",
    "    \n",
    "    dirpath = basecorpus + str(year)\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "        \n",
    "    # timestamps that define window of posts\n",
    "    ts_after = int(dt.datetime(year, 1, 1).timestamp())\n",
    "    ts_before = int(dt.datetime(year+1, 1, 1).timestamp())\n",
    "    \n",
    "    \n",
    "    for subreddit in subreddits:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        action = '\\t[Subeddit]' + subreddit\n",
    "        log_action(action)\n",
    "        \n",
    "        subreddit_dir_path = dirpath + '/' + subreddit\n",
    "        if os.path.exists(subreddit_dir_path):\n",
    "            continue\n",
    "        else:\n",
    "            os.makedirs(subreddit_dir_path)\n",
    "            \n",
    "        submissions_csv_path = str(year) + '-' + subreddit + '-submission.csv'\n",
    "        \n",
    "        submissions_dict = {\n",
    "            \"id\" : [],\n",
    "            \"url\" : [],\n",
    "            \"title\" : [],\n",
    "            \"score\" : [],\n",
    "            \"num_comments\": [],\n",
    "            \"created_utc\" : [],\n",
    "            \"selftext\" : [],\n",
    "        }\n",
    "        \n",
    "        # use PSAW only to get id of submissions in time interval\n",
    "        gen = api.search_submissions(\n",
    "            after=ts_after,\n",
    "            before=ts_before,\n",
    "            filter=['id'],\n",
    "            subreddit=subreddit,\n",
    "            limit=100\n",
    "        )\n",
    "        \n",
    "        # use PRAW to get actual info and traverse comment tree\n",
    "        for submission_psaw in gen:\n",
    "            # use psaw here\n",
    "            submission_id = submission_psaw.d_['id']\n",
    "            # use praw from now on\n",
    "            submission_praw = reddit.submission(id=submission_id)\n",
    "\n",
    "            submissions_dict[\"id\"].append(submission_praw.id)\n",
    "            submissions_dict[\"url\"].append(submission_praw.url)\n",
    "            submissions_dict[\"title\"].append(submission_praw.title)\n",
    "            submissions_dict[\"score\"].append(submission_praw.score)\n",
    "            submissions_dict[\"num_comments\"].append(submission_praw.num_comments)\n",
    "            submissions_dict[\"created_utc\"].append(submission_praw.created_utc)\n",
    "            submissions_dict[\"selftext\"].append(submission_praw.selftext)\n",
    "            \n",
    "            submission_comments_csv_path = str(year) + '-' + subreddit + '-submission_' + submission_id + '-comments.csv'\n",
    "            submission_comments_dict = {\n",
    "                \"comment_id\" : [],\n",
    "                \"comment_parent_id\" : [],\n",
    "                \"comment_body\" : [],\n",
    "                \"comment_link_id\" : [],\n",
    "            }\n",
    "            \n",
    "            # extend the comment tree all the way\n",
    "            submission_praw.comments.replace_more(limit=None)\n",
    "            # for each comment in flattened comment tree\n",
    "            for comment in submission_praw.comments.list():\n",
    "                submission_comments_dict[\"comment_id\"].append(comment.id)\n",
    "                submission_comments_dict[\"comment_parent_id\"].append(comment.parent_id)\n",
    "                submission_comments_dict[\"comment_body\"].append(comment.body)\n",
    "                submission_comments_dict[\"comment_link_id\"].append(comment.link_id)\n",
    "\n",
    "            # for each submission save separate csv comment file\n",
    "            pd.DataFrame(submission_comments_dict).to_csv(subreddit_dir_path + '/' + submission_comments_csv_path,\n",
    "                                                          index=False)\n",
    "            \n",
    "            # single csv file with all submissions\n",
    "            pd.DataFrame(submissions_dict).to_csv(subreddit_dir_path + '/' + submissions_csv_path,\n",
    "                                              index=False)\n",
    "\n",
    "\n",
    "            action = f\"\\t\\t[Info] Found submissions: {pd.DataFrame(submissions_dict).shape[0]}\"\n",
    "            log_action(action)\n",
    "\n",
    "            action = f\"\\t\\t[Info] Elapsed time: {time.time() - start_time: .2f}s\"\n",
    "            log_action(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ep_3_Reddit_Scraping.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
